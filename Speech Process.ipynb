{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COM3502-4502-6502 Speech Processing - Python Programming Assignment\n",
    "\n",
    "## General Information\n",
    "\n",
    "This programming assignment is worth $55$% of the overall course mark.\n",
    "\n",
    "You are free to complete this assignment in your own time. However, feedback, advice and guidance are available during the lab classes and via the discussion board on Blackboard. \n",
    "\n",
    "Note: Via these channels, we try to help you as much as possible, but will not debug your code or provide solutions to the assignment itself.\n",
    "\n",
    "Note: It will take some time to complete this assignment, so plan your work accordingly over the coming weeks. Read these instructions carefully.\n",
    "\n",
    "Note: Please be aware that students registered on COM4502 and COM6502 have **additional tasks** to perform. These are marked ‘COM4502-6502 Only’.\n",
    "\n",
    "Note: You should always ensure that your results (e.g. in terms of plots you create) are clear to understand and leave no room for misinterpretation. This can often be easily achieved by adding proper $x$- and $y$-axis labels, titles, legends etc. Where results are not clear to interpret, this might result in missed points. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Data\n",
    "\n",
    "Student Family Name: <span style=\"font-weight:bold;color:orange\">**Zheng**</span>\n",
    "\n",
    "Student Given Name(s): <span style=\"font-weight:bold;color:orange\">**Fengyuan**</span>\n",
    "\n",
    "Date of submission: <span style=\"font-weight:bold;color:orange\">**Friday 19th December 2024**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright\n",
    "\n",
    "This programming assignment is part of the lecture COM[3502](http://www.dcs.shef.ac.uk/intranet/teaching/public/modules/level3/com3502.html \"Open web page for COM3502 module\")-[4502](http://www.dcs.shef.ac.uk/intranet/teaching/public/modules/level4/com4502.html \"Open web page for COM4502 module\")-[6502](http://www.dcs.shef.ac.uk/intranet/teaching/public/modules/msc/com6502.html \"Open web page for COM4502 module\") Speech Processing at the [University of Sheffield](https://www.sheffield.ac.uk/ \"Open web page of The University of Sheffield\"), Dept. of [Computer Science](https://www.sheffield.ac.uk/dcs \"Open web page of Department of Computer Science\"), University of Sheffield.\n",
    "\n",
    "This notebook is licensed as an assignment to be used during the lecture COM3502-4502-6502 Speech Processing at the University of Sheffield. Any further use is only permitted if agreed with the [module lead](mailto:s.goetze@sheffield.ac.uk).\n",
    "\n",
    "It should be a matter of course that rules of [unfair means](https://www.sheffield.ac.uk/apse/apo/quality/assessment/unfair) apply and the assignment is not to be shared with or made available to other persons besides those participating in the module during the same academic year. This includes publishing on web pages etc. All questions can be asked during the lab classes or using the Blackboard Discussion board.\n",
    "\n",
    "\n",
    "## Hand-In Procedure and Deadline\n",
    "\n",
    "Once you have completed the assignment you should submit a `.zip` file (via Blackboard) containing your solution (as a file named `YourFamilyName.ipynb`) and possibly other sources linked in your Jupyter Notebook. Also, the `.zip` filename should be of the form `YourFamilyName.zip`. Please also ensure that your name is entered correctly in the section above.\n",
    "\n",
    "Standard departmental penalties apply for [late hand-in](https://sites.google.com/sheffield.ac.uk/comughandbook/general-information/assessment/late-submission) and [plagiarism](https://sites.google.com/sheffield.ac.uk/comughandbook/general-information/assessment/unfair-means).\n",
    "\n",
    "The **deadline** for handing-in this assignment (via Blackboard) is \n",
    "<span style=\"font-weight:bold;color:red\">**Friday 19th December 2024**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #abe;\">\n",
    "    \n",
    "**Task 0:**\n",
    "    \n",
    "Ensure that you data is correctly entered in the section at the top of this sheet and that the filename is in the form `YourFamilyName.ipynb`. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "You should be familiar with the use of the following Python libraries from the lab. You should not need to use additional ones. You are allowed to use additional libraries if necessary for your code. If they need to be installed by `!pip install <libraryname>` or `!conda install <libraryname>`, please indicate this as a comment in your code. You should not make use of libraries that can't be installed by either `!pip install` or `!conda install`. You must ensure that your Notebook runs \"out of the box\". You can test this on the Computer Lab machines in the Diamond if you are unsure and using your own computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Let's do some necessary and nice-to-have imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt    # plotting\n",
    "#import seaborn as sns; sns.set()  # styling\n",
    "import numpy as np                 # math\n",
    "import soundfile as sf             # to load files\n",
    "from IPython import display as ipd # for sound playback\n",
    "from scipy import signal           # filter designs (if not already imported)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download, load, and analyse audio\n",
    "\n",
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #abe;\">\n",
    "    \n",
    "**Task T1:** \n",
    "    \n",
    "* Load a wave file containing speech. You can find a file at <a href=\"https://staffwww.dcs.shef.ac.uk/people/S.Goetze/sound/speech.wav\"> https://staffwww.dcs.shef.ac.uk/people/S.Goetze/sound/speech.wav</a> and should be able to download this. You can also use your own WAVE files if you prefer this. If you want to record WAVE files and are using your own computer, the program [Audacity](https://www.audacityteam.org/download/) is one possibility to [record WAVE files](https://manual.audacityteam.org/man/basic_recording_editing_and_exporting.html).\n",
    "    \n",
    "* Visualise the signal in time domain, in the spectral domain (as spectrum) and as a time-frequency representation (spectrogram). Please ensure proper axis labels for all your plot in this assignment.\n",
    "\n",
    "* Playback the signal.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the wave file\n",
    "file_name = 'speech.wav'\n",
    "!curl https://staffwww.dcs.shef.ac.uk/people/S.Goetze/sound/{file_name} -o {file_name}\n",
    "speech_signal, speech_signal_fs = sf.read(file_name)\n",
    "print(speech_signal.shape)\n",
    "print(speech_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualise the signal in time domain\n",
    "speech_signal_time = len(speech_signal)/speech_signal_fs\n",
    "t = np.linspace(0, speech_signal_time, len(speech_signal), endpoint=False)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(t, speech_signal)\n",
    "plt.xlabel('Time in second')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Time domain representation of the speech signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualise the signal in frequency domain, using FFT is faster than DFT\n",
    "signal_length = len(speech_signal)  # the length of the signal is 928086 \n",
    "# we need to find the L_DFT that is the exponential of 2 and should be larger than the length of signal\n",
    "L_DFT = 2**int(np.ceil(np.log2(signal_length)))  #2**20\n",
    "\n",
    "# perform DFT using the FFT algorithm\n",
    "spectrum = np.fft.fft(speech_signal, L_DFT)\n",
    "magnitude_spectrum = np.abs(spectrum)\n",
    "frequency = np.fft.fftfreq(L_DFT, d=1/speech_signal_fs) # the order of frequency is correspond to the output of FFT\n",
    "positive_frequency = frequency[:L_DFT//2]\n",
    "positive_magnitude_spectrum = magnitude_spectrum[:L_DFT//2]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "# plot the full frequency domain of the speech signal\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(frequency, magnitude_spectrum)\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Full frequency domain representation of the speech signal')\n",
    "\n",
    "# plot the positive frequency domain of the speech signal\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(positive_frequency, positive_magnitude_spectrum)\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Positive frequency domain representation of the speech signal')\n",
    "\n",
    "# find the highest occuring frequency\n",
    "max_magnitude_index = np.argmax(positive_magnitude_spectrum)\n",
    "max_frequency = positive_frequency[max_magnitude_index]\n",
    "print(max_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualise the speech signal in time-frequency \n",
    "# small NFFT length leads to high time resolution and low frequency resolution\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.specgram(speech_signal, Fs=speech_signal_fs, NFFT=512, noverlap=256, cmap='viridis', scale='dB')\n",
    "plt.title('spectrogram with DFT length of 512')\n",
    "plt.colorbar(label='dB')\n",
    "plt.xlabel('Time in seconds')\n",
    "plt.ylabel('Frequency in HZ')\n",
    "\n",
    "# large NFFT length leads to low time resolution and high frequency resolution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.specgram(speech_signal, Fs=speech_signal_fs, NFFT=8192, noverlap=4096, cmap='viridis', scale='dB')\n",
    "plt.title('spectrogram with DFT length of 8192')\n",
    "plt.colorbar(label='dB')\n",
    "plt.xlabel('Time in seconds')\n",
    "plt.ylabel('Frequency in HZ')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# playback the speech audio\n",
    "ipd.Audio(speech_signal, rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #9696ff;\">\n",
    "    \n",
    "**Question Q1:** \n",
    "\n",
    "* Determine the sampling frequency $f_s$ in Hz and the length of the signal in seconds. What is the sampling interval $T_s$ of your signal? What is the highest occurring frequency?\n",
    "    \n",
    "Note: You can either give your answer in the form of a code block (e.g. by using the `print()` functions) or as text. For the latter, change the [type of the next cell](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#structure-of-a-notebook-document) from `code` to `markdown` or use the yellow example text below.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-weight:bold;color:orange\">\n",
    "    Sampling frequency $f_s$: Read the speech signal by using soundfile, the sampling frequency $f_s$ is 44100 HZ. <br><br>\n",
    "    Length of the signal (in seconds): The length of the signal can be calculated using:  <br><br>\n",
    "    $$\n",
    "\\text{Signal Length (Seconds)} = \\frac{\\text{Number of Samples}}{\\text{sampling frequency}}\n",
    "    $$ <br><br>\n",
    "    By calculate the formula, we can get signal length is 21.045s to 3 d.p. <br><br>\n",
    "    Sampling interval $T_s$: The sampling interval can be calculated by taking the inverse of sampling frequency $f_s$, which is $2.268\\times 10^{-5}$s to 3 d.p. <br><br>\n",
    "    Highest occuring frequency: the highest frequecy in a signal is determined by the Nyquist freqeucy, which is half of the sampling frequency equal to 20250 HZ. The highest occuring frequency is 176.56 HZ.<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Analysis\n",
    "\n",
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #abe;\">\n",
    "    \n",
    "**Task T2:** \n",
    "    \n",
    "* Generate a synthetic audio signal consisting of three sinusoids with frequencies of $100$ Hz, $300$ Hz, and $500$ Hz including an initial phase which should be different from zero at least for one of the sinusoids. Assume a sampling rate of $8$ kHz and a duration of $2$ seconds. Write code to generate and plot the waveform of this signal. Compute and plot the magnitude spectrum of the signal.\n",
    "    \n",
    "* Use the Fast Fourier Transform (FFT) to calculate the spectrum, and display only the positive frequencies. Identify the main frequency components in the signal (using Python code) based on the magnitude spectrum and briefly explain your observations and identify the three main frequency peaks (as written answer below).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the sampling rate to 8000 and the duration to 2 seconds\n",
    "audio_signal_fs = 8000\n",
    "audio_signal_time = 2\n",
    "audio_signal_t = np.linspace(0, audio_signal_time, int(audio_signal_fs*audio_signal_time), endpoint=False) \n",
    "\n",
    "# for 100HZ with initial phase 0, 300HZ with initial phase π/4, 500HZ with initial phase π/2\n",
    "frequencies = [100, 300, 500]\n",
    "initial_phases = [0, np.pi/4, np.pi/2]\n",
    "audio_signal = (\n",
    "    np.sin(2 * np.pi * frequencies[0] * audio_signal_t + initial_phases[0])\n",
    "    + np.sin(2 * np.pi * frequencies[1] * audio_signal_t + initial_phases[1])\n",
    "    + np.sin(2 * np.pi * frequencies[2] * audio_signal_t + initial_phases[2])\n",
    ")\n",
    "\n",
    "# plot the waveform of synthetic audio signal\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(audio_signal_t, audio_signal)\n",
    "plt.title(\"Waveform of the Synthetic Audio Signal\")\n",
    "plt.xlabel(\"Time in seconds\")\n",
    "plt.ylabel(\"Amplitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to use the fast fourier transformation we need to find the LDFT\n",
    "audio_signal_length = len(audio_signal) # the length of the signal is 16000\n",
    "L_DFT = 2 ** int(np.ceil(np.log2(audio_signal_length)))  # 2**14\n",
    "spectrum = np.fft.fft(audio_signal, L_DFT)\n",
    "magnitude_spectrum = np.abs(spectrum)\n",
    "frequencies = np.fft.fftfreq(L_DFT, d=1/audio_signal_fs)\n",
    "# By the theory of Nyquist, the positive frequency is from 0 to fs/2\n",
    "positive_frequencies = frequencies[:L_DFT//2]\n",
    "positive_magnitude_spectrum = magnitude_spectrum[:L_DFT//2]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "# plot the full frequency domain of the synthetic signal\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(frequencies, magnitude_spectrum)\n",
    "plt.title('Magnitude spectrum of the aduio signal')\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "# plot the positive frequency domain of the synthetic signal\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(positive_frequencies, positive_magnitude_spectrum)\n",
    "plt.title('Magnitude spectrum of the aduio signal with positive frequency')\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top3_amplitude_index = np.argsort(positive_magnitude_spectrum)[-3:]\n",
    "top3_frequency = positive_frequencies[top3_amplitude_index]\n",
    "print(top3_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-weight:bold;color:orange\">According to the code results, the main frequency components in the signal are 299.8 Hz, 100.1 Hz, and 500 Hz. These values appear slightly shifted due to the use of the Discrete Fourier Transform (DFT) for transforming the time-domain signal to the frequency domain. The DFT operates on finite-length signals and has limited frequency resolution, which can cause small deviations (like the decimals observed here). <br><br>\n",
    "From the magnitude spectrum of the audio signal, we can clearly observe three distinct peaks, each corresponding to a frequency component with a high amplitude. These peaks occur at the frequencies of the sinusoidal components used to construct the signal. The absence of significant amplitudes at other frequencies confirms that the signal is composed of exactly three sinusoids. The identified frequency peaks are 100 Hz, 300 Hz, and 500 Hz.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Piece-wise linear filtering in the time domain\n",
    "\n",
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #abe;\">\n",
    "    \n",
    "**Task T3:** \n",
    "    \n",
    "* Design a high-pass filter with a cut-off frequency of $\\approx 500$ Hz using a filter design method of your choice. \n",
    "* Design a low-pass filter with a cut-off frequency of $\\approx 500$ Hz.\n",
    "* Design a band-stop filter with a cut-off frequencies of $\\approx 300$ Hz and of $\\approx 1.1$ kHz.\n",
    "* Simulate the effect of a land-line telephone by eliminating all energy below $300$ Hz and above $3,400$ Hz.\n",
    "* Visualise the transfer functions of the filters and the zero-pole plots.\n",
    "* Apply the designed filters, compare filter input and output as a time-frequency visualisation and play back the filtered signal.\n",
    "    \n",
    "Note: Don't forget proper labeling/description of your figures to make clear what is what.\n",
    "    \n",
    "Note: In case you encounter stability problems, remember that we mentioned in the lecture, that filters can be designed as second-order-systems (SOS) which the design methods you are familiar with can realise.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a signal to test different filters\n",
    "target_rate = 8000\n",
    "num_samples = int(len(speech_signal) * target_rate / speech_signal_fs)\n",
    "resampled_signal = signal.resample(speech_signal, num_samples)\n",
    "resampled_signal_time = len(resampled_signal)/target_rate\n",
    "resampled_t = np.linspace(0, resampled_signal_time, len(resampled_signal), endpoint=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High pass filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Design a high-pass filter with a cut-off frequency of 500HZ by a Butterworth filter\n",
    "filter_sampling_rate = 8000\n",
    "nyquist = 0.5 * filter_sampling_rate\n",
    "cutoff_freq = 500\n",
    "W_act = cutoff_freq / nyquist\n",
    "W_bit = 0.025\n",
    "Wp = W_act + W_bit  # passband edge frequency\n",
    "Ws = W_act - W_bit  # stopband edge frequency\n",
    "Rp_lin = 0.9  # allowed ripples in the pass band area\n",
    "Rs_lin = 0.1  # allowed ripples in the stop band area\n",
    "\n",
    "# draw the tolerance scheme\n",
    "def plot_tolerance_scheme_high_pass(Wp, Ws, Rp_lin, Rs_lin):\n",
    "    dh1x=[0,Wp];  dh1y=[0,0];            \n",
    "    dh2x=[0,Ws];  dh2y=[Rs_lin,Rs_lin];\n",
    "    dv2x=[Ws,Ws]; dv2y=[Rs_lin,1];   \n",
    "    sh1x=[Ws,1];  sh1y=[1,1]; \n",
    "    sh2x=[Wp,1];  sh2y=[Rp_lin,Rp_lin]; \n",
    "    svx=[Wp,Wp];  svy=[0,Rp_lin]; \n",
    "    plt.plot(dh1x,dh1y,'k--',dh2x,dh2y,'k--',dv2x,dv2y,'k--',sh1x,sh1y,'k--',\n",
    "             sh2x,sh2y,'k--',svx,svy,'k--')\n",
    "    plt.xlabel('Frequency $\\Omega/\\pi$')\n",
    "    plt.ylabel('Amplitude $|h(e^{j \\Omega})|$')\n",
    "plot_tolerance_scheme_high_pass(Wp, Ws, Rp_lin, Rs_lin)\n",
    "\n",
    "Rp = -20 * np.log10(Rp_lin)\n",
    "Rs = -20 * np.log10(Rs_lin)\n",
    "N, Wn = signal.buttord(Wp, Ws, Rp, Rs)\n",
    "print('The minimum possible filter order to fulfil the tolerance scheme is '+str(N)+'.')\n",
    "print('The cut-off frequency which will be {:.2f}.'.format(Wn)) # format number - two digits after decimal pt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implement a transfer function visualization plot\n",
    "def plot_transfer_function(b, a, filter_type):\n",
    "    f, h = signal.freqz(b, a)\n",
    "    omega = np.linspace(0, 1, len(f))\n",
    "    plt.plot(omega, np.abs(h), lw=2, label='Butterworth '+str(filter_type)+' filter')\n",
    "    plt.title('Butterworth ' + str(filter_type) + ' filter of order ' + str(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# design high-pass filter of order N using butterworth method\n",
    "b, a = signal.butter(N, Wn, 'high')\n",
    "# plot the transfer function with tolerance scheme\n",
    "plot_tolerance_scheme_high_pass(Wp, Ws, Rp_lin, Rs_lin)\n",
    "plt.plot([Wn,Wn],[0,1],color='red',ls=':',label='cutoff frequency')\n",
    "plot_transfer_function(b=b, a=a, filter_type='high pass')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zplane(z, p, filter_type):\n",
    "    \"Plots zeros and poles in the complex z-plane\"\n",
    "    ax = plt.gca()\n",
    "\n",
    "    ax.plot(np.real(z), np.imag(z), 'bo', fillstyle='none', ms=10)\n",
    "    ax.plot(np.real(p), np.imag(p), 'rx', fillstyle='none', ms=10)\n",
    "    unit_circle = plt.Circle((0, 0), radius=1, fill=False,\n",
    "                             color='black', ls='--', alpha=0.9)\n",
    "    ax.add_patch(unit_circle)\n",
    "\n",
    "    plt.title('Poles and Zeros of ' +str(filter_type)+ ' filter')\n",
    "    plt.xlabel('Re{$z$}')\n",
    "    plt.ylabel('Im{$z$}')\n",
    "    plt.axis('equal')\n",
    "# plot zeros and poles in the z plane\n",
    "zplane(np.roots(b), np.roots(a), filter_type='high pass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "high_pass_filtered_signal = signal.filtfilt(b, a, resampled_signal)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(resampled_t, resampled_signal, color='b', label='resampled signal')\n",
    "plt.plot(resampled_t, high_pass_filtered_signal, color='r', label='filtered signal')\n",
    "plt.xlabel('Time in seconds')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Time domain representation of Resampled and Filtered Signal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "signal_length = len(resampled_signal)  \n",
    "L_DFT = 2**int(np.ceil(np.log2(signal_length)))  \n",
    "\n",
    "# perform DFT using the FFT algorithm\n",
    "spectrum = np.fft.fft(resampled_signal, L_DFT)\n",
    "magnitude_spectrum = np.abs(spectrum)\n",
    "frequency = np.fft.fftfreq(L_DFT, d=1/target_rate) # the order of frequency is correspond to the output of FFT\n",
    "positive_frequency = frequency[:L_DFT//2]\n",
    "positive_magnitude_spectrum = magnitude_spectrum[:L_DFT//2]\n",
    "\n",
    "high_pass_signal_length = len(high_pass_filtered_signal)\n",
    "high_pass_L_DFT = 2**int(np.ceil(np.log2(high_pass_signal_length)))  \n",
    "spectrum_filtered = np.fft.fft(high_pass_filtered_signal, high_pass_L_DFT)\n",
    "magnitude_spectrum_filtered = np.abs(spectrum_filtered)\n",
    "frequency_filtered = np.fft.fftfreq(high_pass_L_DFT, d=1/target_rate) # the order of frequency is correspond to the output of FFT\n",
    "positive_frequency_filtered = frequency_filtered[:high_pass_L_DFT//2]\n",
    "positive_magnitude_spectrum_filtered = magnitude_spectrum_filtered[:high_pass_L_DFT//2]\n",
    "\n",
    "# plot the positive frequency domain of the speech signal\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(positive_frequency, positive_magnitude_spectrum)\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.ylim(0, 500)\n",
    "plt.title('Frequency domain representation of the resampled signal')\n",
    "\n",
    "# plot the positive frequency domain of the filtered speech signal\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(positive_frequency_filtered, positive_magnitude_spectrum_filtered)\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.ylim(0, 500)\n",
    "plt.title('Frequency domain representation of the high pass filtered resampled signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the original and high pass filtered speech signal time-frequency diagram\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.specgram(resampled_signal, Fs=target_rate, NFFT=512, noverlap=256, cmap='viridis', scale='dB', vmin=-240, vmax=-40)\n",
    "plt.title('Original spectrogram with DFT length of 512')\n",
    "plt.colorbar(label='dB')\n",
    "plt.xlabel('Time in seconds')\n",
    "plt.ylabel('Frequency in HZ')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.specgram(high_pass_filtered_signal, Fs=target_rate, NFFT=512, noverlap=256, cmap='viridis', scale='dB', vmin=-240, vmax=-40)\n",
    "plt.title('High pass filtered spectrogram with DFT length of 512')\n",
    "plt.colorbar(label='dB')\n",
    "plt.xlabel('Time in seconds')\n",
    "plt.ylabel('Frequency in HZ')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(resampled_signal, rate=target_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(high_pass_filtered_signal, rate=target_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low pass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Design a low-pass filter with a cut-off frequency of 500HZ by a Butterworth filter\n",
    "filter_sampling_rate = 8000\n",
    "nyquist = 0.5 * filter_sampling_rate\n",
    "cutoff_freq = 500\n",
    "W_act = cutoff_freq / nyquist\n",
    "# we need two frequencies for the tolerance scheme, we chose one of them a bit smaller than W_act and the other a bit larger than o.5, we define \n",
    "# a bit is equal to 0.025\n",
    "W_bit = 0.025\n",
    "Wp = W_act - W_bit  # passband edge frequency\n",
    "Ws = W_act + W_bit  # stopband edge frequency\n",
    "Rp_lin = 0.9  # allowed ripples in the pass band area\n",
    "Rs_lin = 0.1  # allowed ripples in the stop band area\n",
    "\n",
    "# draw the tolerance scheme\n",
    "def plot_tolerance_scheme_low_pass(Wp, Ws, Rp_lin, Rs_lin):\n",
    "    dh1x=[0,Ws];  dh1y=[1,1];            \n",
    "    dh2x=[0,Wp];  dh2y=[Rp_lin,Rp_lin];\n",
    "    dv2x=[Wp,Wp]; dv2y=[0,Rp_lin];   \n",
    "    sh1x=[Ws,1];  sh1y=[Rs_lin,Rs_lin]; \n",
    "    sh2x=[Wp,1];  sh2y=[0,0]; \n",
    "    svx=[Ws,Ws];  svy=[Rs_lin,1];  \n",
    "    # plot the actual lines\n",
    "    plt.plot(dh1x,dh1y,'k--',dh2x,dh2y,'k--',dv2x,dv2y,'k--',sh1x,sh1y,'k--',\n",
    "             sh2x,sh2y,'k--',svx,svy,'k--')\n",
    "    plt.xlabel('Frequency $\\Omega/\\pi$')\n",
    "    plt.ylabel('Amplitude $|h(e^{j \\Omega})|$')\n",
    "plot_tolerance_scheme_low_pass(Wp,Ws,Rp_lin,Rs_lin)\n",
    "\n",
    "Rp = -20 * np.log10(Rp_lin)\n",
    "Rs = -20 * np.log10(Rs_lin)\n",
    "N, Wn = signal.buttord(Wp, Ws, Rp, Rs)\n",
    "print('The minimum possible filter order to fulfil the tolerance scheme is '+str(N)+'.')\n",
    "print('The cut-off frequency which will be {:.2f}.'.format(Wn)) # format number - two digits after decimal pt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# design low-pass filter of order N using butterworth method\n",
    "b, a = signal.butter(N, Wn, 'low')\n",
    "# plot the transfer function with tolerance scheme\n",
    "plot_tolerance_scheme_low_pass(Wp, Ws, Rp_lin, Rs_lin)\n",
    "plt.plot([Wn,Wn],[0,1],color='red',ls=':',label='cutoff frequency')\n",
    "plot_transfer_function(b=b, a=a, filter_type='low pass')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot zeros and poles in the z plane\n",
    "zplane(np.roots(b), np.roots(a), filter_type='low pass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "low_pass_filtered_signal = signal.filtfilt(b, a, resampled_signal)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(resampled_t, resampled_signal, color='b', label='resampled signal')\n",
    "plt.plot(resampled_t, low_pass_filtered_signal, color='r', label='filtered signal')\n",
    "plt.xlabel('Time in seconds')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Time domain representation of Resampled and Filtered Signal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "signal_length = len(resampled_signal)  \n",
    "L_DFT = 2**int(np.ceil(np.log2(signal_length)))  \n",
    "\n",
    "# perform DFT using the FFT algorithm\n",
    "spectrum = np.fft.fft(resampled_signal, L_DFT)\n",
    "magnitude_spectrum = np.abs(spectrum)\n",
    "frequency = np.fft.fftfreq(L_DFT, d=1/target_rate) # the order of frequency is correspond to the output of FFT\n",
    "positive_frequency = frequency[:L_DFT//2]\n",
    "positive_magnitude_spectrum = magnitude_spectrum[:L_DFT//2]\n",
    "\n",
    "low_pass_signal_length = len(low_pass_filtered_signal)\n",
    "low_pass_L_DFT = 2**int(np.ceil(np.log2(low_pass_signal_length)))  \n",
    "spectrum_filtered = np.fft.fft(low_pass_filtered_signal, low_pass_L_DFT)\n",
    "magnitude_spectrum_filtered = np.abs(spectrum_filtered)\n",
    "frequency_filtered = np.fft.fftfreq(low_pass_L_DFT, d=1/target_rate) # the order of frequency is correspond to the output of FFT\n",
    "positive_frequency_filtered = frequency_filtered[:low_pass_L_DFT//2]\n",
    "positive_magnitude_spectrum_filtered = magnitude_spectrum_filtered[:low_pass_L_DFT//2]\n",
    "\n",
    "# plot the positive frequency domain of the speech signal\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(positive_frequency, positive_magnitude_spectrum)\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.ylim(0, 500)\n",
    "plt.title('Frequency domain representation of the resampled signal')\n",
    "\n",
    "# plot the positive frequency domain of the filtered speech signal\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(positive_frequency_filtered, positive_magnitude_spectrum_filtered)\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.ylim(0, 500)\n",
    "plt.title('Frequency domain representation of the low pass filtered resampled signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the original and high pass filtered speech signal time-frequency diagram\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.specgram(resampled_signal, Fs=target_rate, NFFT=512, noverlap=256, cmap='viridis', scale='dB', vmin=-240, vmax=-40)\n",
    "plt.title('Original spectrogram with DFT length of 512')\n",
    "plt.colorbar(label='dB')\n",
    "plt.xlabel('Time in seconds')\n",
    "plt.ylabel('Frequency in HZ')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.specgram(low_pass_filtered_signal, Fs=target_rate, NFFT=512, noverlap=256, cmap='viridis', scale='dB', vmin=-240, vmax=-40)\n",
    "plt.title('Low pass filtered spectrogram with DFT length of 512')\n",
    "plt.colorbar(label='dB')\n",
    "plt.xlabel('Time in seconds')\n",
    "plt.ylabel('Frequency in HZ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(resampled_signal, rate=target_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(low_pass_filtered_signal, rate=target_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Band-stop filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter_sampling_rate = 8000\n",
    "nyquist = 0.5 * filter_sampling_rate\n",
    "cutoff_freq = [300, 1100]\n",
    "W_act = [fc / nyquist for fc in cutoff_freq]\n",
    "# we need two frequencies for the tolerance scheme, we chose one of them a bit smaller than W_act and the other a bit larger than o.5, we define \n",
    "# a bit is equal to 0.025\n",
    "W_bit = 0.025\n",
    "Wp_1 = W_act[0] - W_bit\n",
    "Ws_1 = W_act[0] + W_bit  \n",
    "Wp_2 = W_act[1] + W_bit\n",
    "Ws_2 = W_act[1] - W_bit\n",
    "Wp = [Wp_1, Wp_2]\n",
    "Ws = [Ws_1, Ws_2]\n",
    "Rp_lin = 0.9  # allowed ripples in the pass band area\n",
    "Rs_lin = 0.1  # allowed ripples in the stop band area\n",
    "\n",
    "# draw the tolerance scheme\n",
    "def plot_tolerance_scheme_band_stop(Wp, Ws, Rp_lin, Rs_lin):\n",
    "    dh1x=[0,Ws[0]];  dh1y=[1,1];            \n",
    "    dh2x=[0,Wp[0]];  dh2y=[Rp_lin,Rp_lin];\n",
    "    dh3x=[Ws[1],1];  dh3y=[1,1];\n",
    "    dh4x=[Wp[1],1];  dh4y=[Rp_lin,Rp_lin];\n",
    "    dh5x=[Ws[0],Ws[1]];  dh5y=[Rs_lin, Rs_lin];\n",
    "    dh6x=[Wp[0],Wp[1]];  dh6y=[0,0];\n",
    "    sh1x=[Wp[0],Wp[0]];  sh1y=[Rp_lin,0]; \n",
    "    sh2x=[Ws[0],Ws[0]];  sh2y=[1,Rs_lin]; \n",
    "    sh3x=[Ws[1],Ws[1]]; sh3y=[1,Rs_lin];\n",
    "    sh4x=[Wp[1],Wp[1]]; sh4y=[Rp_lin,0];\n",
    "    # plot the actual lines\n",
    "    plt.plot(dh1x,dh1y,'k--',dh2x,dh2y,'k--',dh3x,dh3y,'k--',dh4x,dh4y,'k--',dh5x,dh5y,'k--',\n",
    "             dh6x,dh6y,'k--',sh1x,sh1y,'k--',sh2x,sh2y,'k--',sh3x,sh3y,'k--',sh4x,sh4y,'k--')\n",
    "    plt.xlabel('Frequency $\\Omega/\\pi$')\n",
    "plot_tolerance_scheme_band_stop(Wp, Ws, Rp_lin, Rs_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Rp = -20 * np.log10(Rp_lin)\n",
    "Rs = -20 * np.log10(Rs_lin)\n",
    "N, Wn = signal.buttord(Wp, Ws, Rp, Rs)\n",
    "print('The minimum possible filter order to fulfil the tolerance scheme is '+str(N)+'.')\n",
    "print('The 1st cut-off frequency which will be {:.2f}.'.format(Wn[0])) \n",
    "print('The 2nd cut-off frequency which will be {:.2f}.'.format(Wn[1]))\n",
    "\n",
    "b, a = signal.butter(N, Wn, 'bandstop')\n",
    "f,h=signal.freqz(b,a)\n",
    "omega=np.linspace(0,1,len(f))\n",
    "\n",
    "# plot the frequency response\n",
    "plot_tolerance_scheme_band_stop(Wp, Ws, Rp_lin, Rs_lin)\n",
    "plt.plot([Wn[0],Wn[0]],[0,1],color='red',ls=':',label='cutoff frequency 1')\n",
    "plt.plot([Wn[1],Wn[1]],[0,1],color='green',ls=':',label='cutoff frequency 2')\n",
    "plt.plot(omega, np.abs(h), lw=2, label='Butterworth bandstop filter')\n",
    "plt.title('Butterworth bandstop filter of order ' + str(N))\n",
    "plt.ylabel('Amplitude $|h(e^{j \\Omega})|$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot zeros and poles in the z plane\n",
    "zplane(np.roots(b), np.roots(a), filter_type='band stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "band_stop_filtered_signal = signal.filtfilt(b, a, resampled_signal)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(resampled_t, resampled_signal, color='b', label='resampled signal')\n",
    "plt.plot(resampled_t, band_stop_filtered_signal, color='r', label='filtered signal')\n",
    "plt.xlabel('Time in seconds')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Time domain representation of Resampled and Filtered Signal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "signal_length = len(resampled_signal)  \n",
    "L_DFT = 2**int(np.ceil(np.log2(signal_length)))  \n",
    "\n",
    "# perform DFT using the FFT algorithm\n",
    "spectrum = np.fft.fft(resampled_signal, L_DFT)\n",
    "magnitude_spectrum = np.abs(spectrum)\n",
    "frequency = np.fft.fftfreq(L_DFT, d=1/target_rate) # the order of frequency is correspond to the output of FFT\n",
    "positive_frequency = frequency[:L_DFT//2]\n",
    "positive_magnitude_spectrum = magnitude_spectrum[:L_DFT//2]\n",
    "\n",
    "band_stop_signal_length = len(band_stop_filtered_signal)\n",
    "band_stop_L_DFT = 2**int(np.ceil(np.log2(band_stop_signal_length)))  \n",
    "spectrum_filtered = np.fft.fft(band_stop_filtered_signal, band_stop_L_DFT)\n",
    "magnitude_spectrum_filtered = np.abs(spectrum_filtered)\n",
    "frequency_filtered = np.fft.fftfreq(band_stop_L_DFT, d=1/target_rate) # the order of frequency is correspond to the output of FFT\n",
    "positive_frequency_filtered = frequency_filtered[:band_stop_L_DFT//2]\n",
    "positive_magnitude_spectrum_filtered = magnitude_spectrum_filtered[:band_stop_L_DFT//2]\n",
    "\n",
    "# plot the positive frequency domain of the speech signal\n",
    "plt.figure(figsize=(17, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(positive_frequency, positive_magnitude_spectrum)\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.ylim(0, 500)\n",
    "plt.title('Frequency domain representation of the resampled signal')\n",
    "\n",
    "# plot the positive frequency domain of the filtered speech signal\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(positive_frequency_filtered, positive_magnitude_spectrum_filtered)\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.ylim(0, 500)\n",
    "plt.title('Frequency domain representation of the band stop filtered resampled signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the original and band stop filtered speech signal time-frequency diagram\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.specgram(resampled_signal, Fs=target_rate, NFFT=512, noverlap=256, cmap='viridis', scale='dB', vmin=-240, vmax=-40)\n",
    "plt.title('Original spectrogram with DFT length of 512')\n",
    "plt.colorbar(label='dB')\n",
    "plt.xlabel('Time in seconds')\n",
    "plt.ylabel('Frequency in HZ')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.specgram(band_stop_filtered_signal, Fs=target_rate, NFFT=512, noverlap=256, cmap='viridis', scale='dB', vmin=-240, vmax=-40)\n",
    "plt.title('Band stop filtered spectrogram with DFT length of 512')\n",
    "plt.colorbar(label='dB')\n",
    "plt.xlabel('Time in seconds')\n",
    "plt.ylabel('Frequency in HZ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(resampled_signal, rate=target_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(band_stop_filtered_signal, rate=target_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Band pass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter_sampling_rate = 8000\n",
    "nyquist = 0.5 * filter_sampling_rate\n",
    "cutoff_freq = [300, 3400]\n",
    "W_act = [fc / nyquist for fc in cutoff_freq]\n",
    "# we need two frequencies for the tolerance scheme, we chose one of them a bit smaller than W_act and the other a bit larger than o.5, we define \n",
    "# a bit is equal to 0.025\n",
    "W_bit = 0.025\n",
    "Wp_1 = W_act[0] + W_bit \n",
    "Ws_1 = W_act[0] - W_bit  \n",
    "Wp_2 = W_act[1] - W_bit\n",
    "Ws_2 = W_act[1] + W_bit\n",
    "Wp = [Wp_1, Wp_2]\n",
    "Ws = [Ws_1, Ws_2]\n",
    "Rp_lin = 0.9  # allowed ripples in the pass band area\n",
    "Rs_lin = 0.1  # allowed ripples in the stop band area\n",
    "\n",
    "# draw the tolerance scheme\n",
    "def plot_tolerance_scheme_band_pass(Wp, Ws, Rp_lin, Rs_lin):\n",
    "    dh1x=[0,Ws[0]];  dh1y=[Rs_lin,Rs_lin];            \n",
    "    dh2x=[0,Wp[0]];  dh2y=[0,0];\n",
    "    dh3x=[Ws[1],1];  dh3y=[Rs_lin,Rs_lin];\n",
    "    dh4x=[Wp[1],1];  dh4y=[0,0];\n",
    "    dh5x=[Ws[0],Ws[1]];  dh5y=[1,1];\n",
    "    dh6x=[Wp[0],Wp[1]];  dh6y=[Rp_lin,Rp_lin];\n",
    "    sh1x=[Wp[0],Wp[0]];  sh1y=[Rp_lin,0]; \n",
    "    sh2x=[Ws[0],Ws[0]];  sh2y=[1,Rs_lin]; \n",
    "    sh3x=[Ws[1],Ws[1]]; sh3y=[1,Rs_lin];\n",
    "    sh4x=[Wp[1],Wp[1]]; sh4y=[Rp_lin,0];\n",
    "    # plot the actual lines\n",
    "    plt.plot(dh1x,dh1y,'k--',dh2x,dh2y,'k--',dh3x,dh3y,'k--',dh4x,dh4y,'k--',dh5x,dh5y,'k--',\n",
    "             dh6x,dh6y,'k--',sh1x,sh1y,'k--',sh2x,sh2y,'k--',sh3x,sh3y,'k--',sh4x,sh4y,'k--')\n",
    "    plt.xlabel('Frequency $\\Omega/\\pi$')\n",
    "plot_tolerance_scheme_band_pass(Wp, Ws, Rp_lin, Rs_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Rp = -20 * np.log10(Rp_lin)\n",
    "Rs = -20 * np.log10(Rs_lin)\n",
    "N, Wn = signal.buttord(Wp, Ws, Rp, Rs)\n",
    "print('The minimum possible filter order to fulfil the tolerance scheme is '+str(N)+'.')\n",
    "print('The 1st cut-off frequency which will be {:.2f}.'.format(Wn[0])) \n",
    "print('The 2nd cut-off frequency which will be {:.2f}.'.format(Wn[1]))\n",
    "\n",
    "b, a = signal.butter(N, Wn, 'band')\n",
    "f,h=signal.freqz(b,a)\n",
    "omega=np.linspace(0,1,len(f))\n",
    "\n",
    "# plot the frequency response\n",
    "plot_tolerance_scheme_band_pass(Wp, Ws, Rp_lin, Rs_lin)\n",
    "plt.plot([Wn[0],Wn[0]],[0,1],color='red',ls=':',label='cutoff frequency 1')\n",
    "plt.plot([Wn[1],Wn[1]],[0,1],color='green',ls=':',label='cutoff frequency 2')\n",
    "plt.plot(omega, np.abs(h), lw=2, label='Butterworth bandpass filter')\n",
    "plt.title('Butterworth bandpass filter of order ' + str(N))\n",
    "plt.ylabel('Amplitude $|h(e^{j \\Omega})|$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot zeros and poles in the z plane\n",
    "zplane(np.roots(b), np.roots(a), filter_type='band pass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "band_pass_filtered_signal = signal.filtfilt(b, a, resampled_signal)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(resampled_t, resampled_signal, color='b', label='resampled signal')\n",
    "plt.plot(resampled_t, band_pass_filtered_signal, color='r', label='filtered signal')\n",
    "plt.xlabel('Time in seconds')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Time domain representation of Resampled and Filtered Signal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "signal_length = len(resampled_signal)  \n",
    "L_DFT = 2**int(np.ceil(np.log2(signal_length)))  \n",
    "\n",
    "# perform DFT using the FFT algorithm\n",
    "spectrum = np.fft.fft(resampled_signal, L_DFT)\n",
    "magnitude_spectrum = np.abs(spectrum)\n",
    "frequency = np.fft.fftfreq(L_DFT, d=1/target_rate) # the order of frequency is correspond to the output of FFT\n",
    "positive_frequency = frequency[:L_DFT//2]\n",
    "positive_magnitude_spectrum = magnitude_spectrum[:L_DFT//2]\n",
    "\n",
    "band_pass_signal_length = len(band_pass_filtered_signal)\n",
    "band_pass_L_DFT = 2**int(np.ceil(np.log2(band_pass_signal_length)))  \n",
    "spectrum_filtered = np.fft.fft(band_pass_filtered_signal, band_pass_L_DFT)\n",
    "magnitude_spectrum_filtered = np.abs(spectrum_filtered)\n",
    "frequency_filtered = np.fft.fftfreq(band_pass_L_DFT, d=1/target_rate) # the order of frequency is correspond to the output of FFT\n",
    "positive_frequency_filtered = frequency_filtered[:band_pass_L_DFT//2]\n",
    "positive_magnitude_spectrum_filtered = magnitude_spectrum_filtered[:band_pass_L_DFT//2]\n",
    "\n",
    "# plot the positive frequency domain of the speech signal\n",
    "plt.figure(figsize=(17, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(positive_frequency, positive_magnitude_spectrum)\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.ylim(0, 500)\n",
    "plt.title('Frequency domain representation of the resampled signal')\n",
    "\n",
    "# plot the positive frequency domain of the filtered speech signal\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(positive_frequency_filtered, positive_magnitude_spectrum_filtered)\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.ylim(0, 500)\n",
    "plt.title('Frequency domain representation of the band pass filtered resampled signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the original and band pass filtered speech signal time-frequency diagram\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.specgram(resampled_signal, Fs=target_rate, NFFT=512, noverlap=256, cmap='viridis', scale='dB', vmin=-240, vmax=-40)\n",
    "plt.title('Original spectrogram with DFT length of 512')\n",
    "plt.colorbar(label='dB')\n",
    "plt.xlabel('Time in seconds')\n",
    "plt.ylabel('Frequency in HZ')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.specgram(band_pass_filtered_signal, Fs=target_rate, NFFT=512, noverlap=256, cmap='viridis', scale='dB', vmin=-240, vmax=-40)\n",
    "plt.title('Band pass filtered spectrogram with DFT length of 512')\n",
    "plt.colorbar(label='dB')\n",
    "plt.xlabel('Time in seconds')\n",
    "plt.ylabel('Frequency in HZ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(resampled_signal, rate=target_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(band_pass_filtered_signal, rate=target_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #9696ff;\">\n",
    "    \n",
    "**Question Q2:** \n",
    "\n",
    "* Explain the behaviour of the designed band-stop filter, i.e. describe (briefly) what you can see in the generated plots. If you didn't generate plots you can explain what you would expect to see.\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Question Q2:**\n",
    "\n",
    "<span style=\"font-weight:bold;color:orange\">\n",
    "A band-stop filter attenuates frequencies between two cut-off frequencies (defining the stop-band) while allowing frequencies outside this range (in the lower and higher pass-bands) to pass through with little attenuation. <br><br>\n",
    "According to the Poles-Zeros graph, we observed that there are two zeros placed on the unit circle, representing the stop-band where frequencies are attenuated. The poles are located inside the unit circle, ensure stability and maintain the pass-band behaviour, allowing signals outside the stop-band to pass through with little attenuation.<br><br>\n",
    "According to the frequency-domain graph, the frequencies withinn the stop-band (300-1100 HZ) have been significantly attenuated, the frequencies below 300HZ or above 1100 HZ remain largely affected. This behavior is consistent with the expected behavior of a band-stop filter.<br><br>\n",
    "According to the time-frequency graph, frequencies within the stop-band (300-1100 HZ) have been significantly attenuated, as shown by the dark blue regions indicating lower levels in dB. Frequencies outside the stop-bandremain relatively unaffected, as shown by brighter region.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #9696ff;\">\n",
    "    \n",
    "**Question Q3:**\n",
    "    \n",
    "* Which sounds are most affected when the low-pass cut-off frequency is set to around $500$\n",
    "Hz - vowels or consonants - and why?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Question Q3:**\n",
    "\n",
    "<span style=\"font-weight:bold;color:orange\">\n",
    "Consonants are most affected when the low pass cut-off frequency is set to around 500 HZ. Vowels are primarily composed of low-frequency components. The fundamental frequency of vowels and the first formant for most vowels like /i/, /u/, /ʊ/ and /ɛ/ lie below 500 HZ. These low frequencies are relatively unaffected by a 500 HZ low-pass filter, allowing vowels to remain recognizable even if higher formants (which may exceed 500 Hz) are attenuated. However, consonants contain a significant amount of high-frequency components, usually distributed between 1000 HZ to 8000 HZ, which are largly elimiated by 500 low pass filter, making consonants hard to hear or recognize. Therefore, consonants are most affected by a 500 HZ low pass filter.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Effects\n",
    "\n",
    "## Low-Frequency Oscillator\n",
    "\n",
    "Many ‘voice effects (FXs)’ are achieved by modifying some characteristic of the speech using a low-frequency oscillator or *LFO*. LFOs typically have two controls: speed (which is specified by the frequency in Hertz) and depth (which specifies the magnitude of the effect). The following tasks will require several LFOs, so it makes sense to implement one in the following.\n",
    "\n",
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #abe;\">\n",
    "    \n",
    "**Task T4:**\n",
    "    \n",
    "* Implement a Low Frequency Oscillator as a function `lfo()` as described below. Visualise that your function works by generating a sine and a square wave of frequency $5$ Hz and length $2$ seconds with different depths.\n",
    "    \n",
    "Note: There will be an extra point in the marking if you **do not** use the `scipy` library to solve this task.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lfo_no_scipy(speed_hz, depth, num_samples, fs=44100, square_curve=False):\n",
    "    '''\n",
    "    Low-frequency oscillator\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    speed_hz : float\n",
    "       frequency of generated signal in Hertz\n",
    "    depth : float\n",
    "        magnitude of the effect\n",
    "    num_samples : int\n",
    "        length of the signal in samples\n",
    "    fs : float, optional \n",
    "        sampling frequency in Hz, default 44100\n",
    "    square_curve : boolean, optional (default: False)\n",
    "        generate square wave if true, generate sine wave if false\n",
    "\n",
    "    Example use:\n",
    "    -------\n",
    "        sig_square = lfo(speed_hz=5, depth=0.7, num_samples=88200, fs=44100, square_curve=True)\n",
    "    '''\n",
    "    # Generate time vector\n",
    "    t = np.linspace(0, int(num_samples/fs), num_samples, endpoint=False)  \n",
    "    if square_curve:\n",
    "        lfo_signal = depth * np.sign(np.sin(2 * np.pi * speed_hz * t))\n",
    "    else:\n",
    "        lfo_signal = depth * np.sin(2 * np.pi * speed_hz * t)\n",
    "        \n",
    "    return lfo_signal, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use lfo_no_scipy function to visualize LFO\n",
    "fs = 44100  \n",
    "duration = 2\n",
    "num_samples = int(fs * duration)\n",
    "speed_hz = 5  # LFO frequency (5 Hz)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "# Generate sine signal with amplitude 0.5 (depth=0.5)\n",
    "plt.subplot(3, 2, 1)\n",
    "sine_wave_half, sine_wave_half_t = lfo_no_scipy(speed_hz=speed_hz, depth=0.5, num_samples=num_samples, fs=fs, square_curve=False)\n",
    "plt.plot(sine_wave_half_t, sine_wave_half)\n",
    "plt.title('Sine wave with amplitude 0.5')\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid()\n",
    "\n",
    "# Generate sine signal with amplitude 1 (depth=1)\n",
    "plt.subplot(3, 2, 3)\n",
    "sine_wave_1, sine_wave_1_t = lfo_no_scipy(speed_hz=speed_hz, depth=1, num_samples=num_samples, fs=fs, square_curve=False)\n",
    "plt.plot(sine_wave_1_t, sine_wave_1)\n",
    "plt.title('Sine wave with amplitude 1')\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid()\n",
    "\n",
    "# Generate sine signal with amplitude 2 (depth=2)\n",
    "plt.subplot(3, 2, 5)\n",
    "sine_wave_2, sine_wave_2_t = lfo_no_scipy(speed_hz=speed_hz, depth=2, num_samples=num_samples, fs=fs, square_curve=False)\n",
    "plt.plot(sine_wave_2_t, sine_wave_2)\n",
    "plt.title('Sine wave with amplitude 2')\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid()\n",
    "\n",
    "# Generate square signal with amplitude 0.5 (depth=0.5)\n",
    "plt.subplot(3, 2, 2)\n",
    "square_wave_half, square_wave_half_t = lfo_no_scipy(speed_hz=speed_hz, depth=0.5, num_samples=num_samples, fs=fs, square_curve=True)\n",
    "plt.plot(square_wave_half_t, square_wave_half)\n",
    "plt.title('Square wave with amplitude 0.5')\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid()\n",
    "\n",
    "# Generate square signal with amplitude 1 (depth=1)\n",
    "plt.subplot(3, 2, 4)\n",
    "square_wave_1, square_wave_1_t = lfo_no_scipy(speed_hz=speed_hz, depth=1, num_samples=num_samples, fs=fs, square_curve=True)\n",
    "plt.plot(square_wave_1_t, square_wave_1)\n",
    "plt.title('Square wave with amplitude 1')\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid()\n",
    "\n",
    "# Generate square signal with amplitude 2 (depth=2)\n",
    "plt.subplot(3, 2, 6)\n",
    "square_wave_2, square_wave_2_t = lfo_no_scipy(speed_hz=speed_hz, depth=2, num_samples=num_samples, fs=fs, square_curve=True)\n",
    "plt.plot(square_wave_2_t, square_wave_2)\n",
    "plt.title('Square wave with amplitude 2')\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although your function outputs audio, you are unlikely to be able to hear it as the frequency is so low. However, you can check that it is functioning correctly by combining it with other audio signals as we will do in the following.\n",
    "\n",
    "## Amplitude Modulation - Tremolo\n",
    "\n",
    "*Tremolo* is one of the most basic voice manipulations that makes use of an LFO. In this effect, the amplitude of a speech signal is [modulated](https://en.wikipedia.org/wiki/Amplitude_modulation), i.e. the speech waveform is multiplied by a variable gain that ranges between $1-$ `modulation_depth` and $1$. This means that if the `modulation_depth` equals $1$, the variable gain varies between $1-1=0$ and $1$. \n",
    "\n",
    "Your LFO outputs an audio signal between `-depth` and `+depth` which is different from the `modulation_depth` above. So, in order to modulate the amplitude of the speech correctly, the output of the LFO has to be scaled and applied appropriately.\n",
    "\n",
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #abe;\">\n",
    "    \n",
    "**Task T5:**\n",
    "    \n",
    "* Implement  a function `tremolo()` using your function `lfo()` and modulate the amplitude of the speech signal. \n",
    "* Experiment with different settings for `speed` and `modulation_depth`. In particular, note that a square wave with a *speed* between $3$ and $4$ Hz (and `modulation_depth` = $1$) has a very destructive effect on the intelligibility of the output. This is because $3-4$ Hz corresponds to the typical syllabic rate of speech.\n",
    "* Proof that the effect works by a proper visualisation of the filtered speech signal and describe what can be observed and perceived.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tremolo(signal, fs, speed_hz, modulation_depth, square_curve=False):\n",
    "    '''\n",
    "    Applies a tremolo effect to a signal\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : float\n",
    "       input signal to which the effect should be applied\n",
    "    fs : int\n",
    "        sampling frequency in Hz\n",
    "    speed_hz : float\n",
    "       frequency of LFO and by this also the effect\n",
    "    modulation_depth : float\n",
    "        magnitude of the effect\n",
    "    square_curve : boolean, optional\n",
    "        generate square wave if true, generate sine wave if false\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    signal after application of tremolo effect\n",
    "    \n",
    "    Example use:\n",
    "    -------\n",
    "        signal_tremolo = tremolo(signal=audio_in, fs=fs, speed_hz=10, modulation_depth=1, square_curve=False)\n",
    "    '''\n",
    "    \n",
    "    number_samples = len(signal)\n",
    "    lfo_output = lfo_no_scipy(speed_hz=speed_hz, depth=modulation_depth, num_samples=number_samples, fs=fs, square_curve=square_curve)[0]\n",
    "    lfo_output_scaled = (lfo_output + modulation_depth)/(2 * modulation_depth)\n",
    "    variable_gain = 1 - modulation_depth + modulation_depth * lfo_output_scaled\n",
    "    tremolo_signal = signal * variable_gain\n",
    "    return tremolo_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the original speech signal time domain diagram\n",
    "speech_signal_time = len(speech_signal)/speech_signal_fs\n",
    "t = np.linspace(0, speech_signal_time, len(speech_signal), endpoint=False)\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(t, speech_signal)\n",
    "plt.xlabel('Time in second')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Time domain representation of the signal')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Changing speed_hz to show the effect of Tremolo signal\n",
    "plt.figure(figsize=(18, 5*3))\n",
    "speed_hz_list = [1, 5, 10]\n",
    "tremelo_speed_sign_list = []\n",
    "for i, speed_hz in enumerate(speed_hz_list):\n",
    "    plt.subplot(3, 3, i*3+1)\n",
    "    tremolo_signal = tremolo(signal=speech_signal, fs=speech_signal_fs, speed_hz=speed_hz, modulation_depth=1, square_curve=True)\n",
    "    tremelo_speed_sign_list.append(tremolo_signal)\n",
    "    plt.plot(t, tremolo_signal)\n",
    "    plt.title(f\"Tremolo signal\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    signal_length = len(tremolo_signal)  \n",
    "    L_DFT = 2**int(np.ceil(np.log2(signal_length)))  \n",
    "    # perform DFT using the FFT algorithm\n",
    "    spectrum = np.fft.fft(tremolo_signal, L_DFT)\n",
    "    magnitude_spectrum = np.abs(spectrum)\n",
    "    frequency = np.fft.fftfreq(L_DFT, d=1/speech_signal_fs) \n",
    "    positive_frequency = frequency[:L_DFT//2]\n",
    "    positive_magnitude_spectrum = magnitude_spectrum[:L_DFT//2]\n",
    "\n",
    "    # plot the positive frequency domain of the speech signal\n",
    "    plt.subplot(3, 3, i*3+2)\n",
    "    plt.plot(positive_frequency, positive_magnitude_spectrum)\n",
    "    plt.xlabel('Frequency in HZ')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title('Frequency domain representation of the tremolo signal')\n",
    "\n",
    "    # plot the tremolo signal time-frequency diagram\n",
    "    plt.subplot(3, 3, i*3+3)\n",
    "    # avoid divided by 0, we add a small number \n",
    "    plt.specgram(tremolo_signal+1e-10, Fs=speech_signal_fs, NFFT=512, noverlap=256, cmap='viridis', scale='dB')\n",
    "    plt.title('Original spectrogram with DFT length of 512')\n",
    "    plt.colorbar(label='dB')\n",
    "    plt.xlabel('Time in seconds')\n",
    "    plt.ylabel('Frequency in HZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Changing depth to show the effect of Tremolo signal\n",
    "plt.figure(figsize=(18, 5*3))\n",
    "depth_list = [1, 5, 10]\n",
    "tremelo_depth_sign_list = []\n",
    "for i, depth in enumerate(speed_hz_list):\n",
    "    plt.subplot(3, 3, i*3+1)\n",
    "    tremolo_signal = tremolo(signal=speech_signal, fs=speech_signal_fs, speed_hz=5, modulation_depth=depth, square_curve=True)\n",
    "    tremelo_speed_sign_list.append(tremolo_signal)\n",
    "    plt.plot(t, tremolo_signal)\n",
    "    plt.title(f\"Tremolo signal\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    signal_length = len(tremolo_signal)  \n",
    "    L_DFT = 2**int(np.ceil(np.log2(signal_length)))  \n",
    "    # perform DFT using the FFT algorithm\n",
    "    spectrum = np.fft.fft(tremolo_signal, L_DFT)\n",
    "    magnitude_spectrum = np.abs(spectrum)\n",
    "    frequency = np.fft.fftfreq(L_DFT, d=1/speech_signal_fs) \n",
    "    positive_frequency = frequency[:L_DFT//2]\n",
    "    positive_magnitude_spectrum = magnitude_spectrum[:L_DFT//2]\n",
    "\n",
    "    # plot the positive frequency domain of the speech signal\n",
    "    plt.subplot(3, 3, i*3+2)\n",
    "    plt.plot(positive_frequency, positive_magnitude_spectrum)\n",
    "    plt.xlabel('Frequency in HZ')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title('Frequency domain representation of the tremolo signal')\n",
    "\n",
    "    # plot the tremolo signal time-frequency diagram\n",
    "    plt.subplot(3, 3, i*3+3)\n",
    "    # avoid divided by 0, we add a small number \n",
    "    plt.specgram(tremolo_signal+1e-10, Fs=speech_signal_fs, NFFT=512, noverlap=256, cmap='viridis', scale='dB')\n",
    "    plt.title('Original spectrogram with DFT length of 512')\n",
    "    plt.colorbar(label='dB')\n",
    "    plt.xlabel('Time in seconds')\n",
    "    plt.ylabel('Frequency in HZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show the destructive effect of square wave with a speed between 3 and 4 Hz, depth=1\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "tremolo_signal = tremolo(signal=speech_signal, fs=speech_signal_fs, speed_hz=3.5, modulation_depth=1, square_curve=True)\n",
    "plt.plot(t, tremolo_signal)\n",
    "plt.title(f\"Tremolo signal\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "\n",
    "signal_length = len(tremolo_signal)  \n",
    "L_DFT = 2**int(np.ceil(np.log2(signal_length)))  \n",
    "# perform DFT using the FFT algorithm\n",
    "spectrum = np.fft.fft(tremolo_signal, L_DFT)\n",
    "magnitude_spectrum = np.abs(spectrum)\n",
    "frequency = np.fft.fftfreq(L_DFT, d=1/speech_signal_fs) \n",
    "positive_frequency = frequency[:L_DFT//2]\n",
    "positive_magnitude_spectrum = magnitude_spectrum[:L_DFT//2]\n",
    "\n",
    "# plot the positive frequency domain of the speech signal\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(positive_frequency, positive_magnitude_spectrum)\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Frequency domain representation of the tremolo signal')\n",
    "\n",
    "# plot the tremolo signal time-frequency diagram\n",
    "plt.subplot(1, 3, 3)\n",
    "# avoid divided by 0, we add a small number \n",
    "plt.specgram(tremolo_signal+1e-10, Fs=speech_signal_fs, NFFT=512, noverlap=256, cmap='viridis', scale='dB')\n",
    "plt.title('Original spectrogram with DFT length of 512')\n",
    "plt.colorbar(label='dB')\n",
    "plt.xlabel('Time in seconds')\n",
    "plt.ylabel('Frequency in HZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(tremolo_signal, rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-weight:bold;color:orange\">\n",
    "The left plot shows the amplitude of the speech signal modulated by the tremolo effect over time. It represents the effect of a tremolo with speed_hz=3.5 and depth=1. We can observe periodic fluctuations in amplitude, which are characteristic of the tremolo effect. The tremolo signal has regions where the amplitude decreases significantly, which disrupts the continuity of the speech, leading to perceptual distortion.<br><br>\n",
    "\n",
    "The middle plot shows the frequency domian representation of the speech signal modulated by the tremolo effect over time. Since the frequency domain is very wide, we cannot directly observe obvious effect. We can perceive additional low-frequency content.<br><br>\n",
    "\n",
    "The right plot shows the spectrogram of the tremolo signal.The spectrogram reveals a periodic weakening and strengthening of frequencies (we can see alternating dark and bright vertical lines), corresponding to the amplitude modulation caused by the tremolo effect.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ring Modulation\n",
    "\n",
    "Another basic effect is to multiply the speech signal by the output of an LFO. This is known as ‘ring modulation’.\n",
    "\n",
    "Note: In the BBC TV series [Dr. Who](https://en.wikipedia.org/wiki/Doctor_Who), the voices of the alien [Daleks](https://en.wikipedia.org/wiki/Dalek) are generated by a ring modulator with an LFO set to around 30 Hz. The voice actors also spoke using a stilted monotonic intonation in order to enhance the effect. You can try this yourself by recording your own voice and applying the effect.\n",
    "\n",
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #abe;\">\n",
    "    \n",
    "**Task T6 (Ring Modulation):**\n",
    "    \n",
    "* Implement a function `ring_modulation()` using your function `lfo()` and modulate the amplitude of the speech signal by multiplying with the LFO signal. \n",
    "* Experiment with different settings for `speed` and `depth`. Note how the timbre of the resulting sound is subtly different from *tremolo*.\n",
    "* Proof that the effect works by a proper visualisation of the filtered speech signal and describe what can be observed and perceived.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ring_modulation(signal, fs, speed_hz, depth, square_curve=False):\n",
    "    '''\n",
    "    Applies a ring modulation effect to a signal\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : float\n",
    "       input signal to which the effect should be applied\n",
    "    fs : int\n",
    "        sampling frequency in Hz\n",
    "    speed_hz : float\n",
    "       frequency of LFO and by this also the effect\n",
    "    depth : float\n",
    "        magnitude of the effect\n",
    "    square_curve : boolean, optional\n",
    "        generate square wave if true, generate sine wave if false\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    signal after application of the ring modulation effect\n",
    "    \n",
    "    Example use:\n",
    "    -------\n",
    "        signal_ring_mod = ring_modulation(audio_in, fs, 10, 1, square_curve=False)\n",
    "    '''\n",
    "    num_samples = len(signal)\n",
    "    lfo_output = lfo_no_scipy(speed_hz=speed_hz, depth=depth, num_samples=num_samples, fs=fs, square_curve=square_curve)[0]\n",
    "    ring_modulation_signal = signal * lfo_output\n",
    "    return ring_modulation_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here to show an example of the Ring Modulation effect\n",
    "# plot the original speech signal time domain diagram\n",
    "speech_signal_time = len(speech_signal)/speech_signal_fs\n",
    "t = np.linspace(0, speech_signal_time, len(speech_signal), endpoint=False)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(t, speech_signal)\n",
    "plt.xlabel('Time in second')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Time domain representation of the signal')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Changing speed_hz to show the effect of Ring Modulation\n",
    "plt.figure(figsize=(18, 5*3))\n",
    "speed_hz_list = [1, 5, 10]\n",
    "ring_speed_sign_list = []\n",
    "for i, speed_hz in enumerate(speed_hz_list):\n",
    "    plt.subplot(3, 3, i*3+1)\n",
    "    ring_signal = ring_modulation(signal=speech_signal, fs=speech_signal_fs, speed_hz=speed_hz, depth=1, square_curve=False)\n",
    "    ring_speed_sign_list.append(ring_signal)\n",
    "    plt.plot(t, ring_signal)\n",
    "    plt.title(f\"Ring signal\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    signal_length = len(ring_signal)  \n",
    "    L_DFT = 2**int(np.ceil(np.log2(signal_length)))  \n",
    "    # perform DFT using the FFT algorithm\n",
    "    spectrum = np.fft.fft(ring_signal, L_DFT)\n",
    "    magnitude_spectrum = np.abs(spectrum)\n",
    "    frequency = np.fft.fftfreq(L_DFT, d=1/speech_signal_fs) \n",
    "    positive_frequency = frequency[:L_DFT//2]\n",
    "    positive_magnitude_spectrum = magnitude_spectrum[:L_DFT//2]\n",
    "\n",
    "    # plot the positive frequency domain of the speech signal\n",
    "    plt.subplot(3, 3, i*3+2)\n",
    "    plt.plot(positive_frequency, positive_magnitude_spectrum)\n",
    "    plt.xlabel('Frequency in HZ')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title('Frequency domain representation of the ring signal')\n",
    "\n",
    "    # plot the ring signal time-frequency diagram\n",
    "    plt.subplot(3, 3, i*3+3)\n",
    "    # avoid divided by 0, we add a small number \n",
    "    plt.specgram(ring_signal, Fs=speech_signal_fs, NFFT=512, noverlap=256, cmap='viridis', scale='dB')\n",
    "    plt.title('Original spectrogram with DFT length of 512')\n",
    "    plt.colorbar(label='dB')\n",
    "    plt.xlabel('Time in seconds')\n",
    "    plt.ylabel('Frequency in HZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5*3))\n",
    "depth_list = [1, 5, 10]\n",
    "ring_depth_sign_list = []\n",
    "for i, depth in enumerate(depth_list):\n",
    "    plt.subplot(3, 3, i*3+1)\n",
    "    ring_signal = ring_modulation(signal=speech_signal, fs=speech_signal_fs, speed_hz=5, depth=depth, square_curve=False)\n",
    "    ring_depth_sign_list.append(ring_signal)\n",
    "    plt.plot(t, ring_signal)\n",
    "    plt.title(f\"Ring signal\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    signal_length = len(ring_signal)  \n",
    "    L_DFT = 2**int(np.ceil(np.log2(signal_length)))  \n",
    "    # perform DFT using the FFT algorithm\n",
    "    spectrum = np.fft.fft(ring_signal, L_DFT)\n",
    "    magnitude_spectrum = np.abs(spectrum)\n",
    "    frequency = np.fft.fftfreq(L_DFT, d=1/speech_signal_fs) \n",
    "    positive_frequency = frequency[:L_DFT//2]\n",
    "    positive_magnitude_spectrum = magnitude_spectrum[:L_DFT//2]\n",
    "\n",
    "    # plot the positive frequency domain of the speech signal\n",
    "    plt.subplot(3, 3, i*3+2)\n",
    "    plt.plot(positive_frequency, positive_magnitude_spectrum)\n",
    "    plt.xlabel('Frequency in HZ')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title('Frequency domain representation of the ring signal')\n",
    "\n",
    "    # plot the ring signal time-frequency diagram\n",
    "    plt.subplot(3, 3, i*3+3)\n",
    "    # avoid divided by 0, we add a small number \n",
    "    plt.specgram(ring_signal, Fs=speech_signal_fs, NFFT=512, noverlap=256, cmap='viridis', scale='dB')\n",
    "    plt.title('Original spectrogram with DFT length of 512')\n",
    "    plt.colorbar(label='dB')\n",
    "    plt.xlabel('Time in seconds')\n",
    "    plt.ylabel('Frequency in HZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(ring_depth_sign_list[2], rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Shifting\n",
    "\n",
    "Many Vocal FX are the result of altering the frequencies present, e.g. changing the pitch of a voice. There are many algorithms for frequency shifting. You have already implemented an approximate solution with your ring modulator.\n",
    "\n",
    "For simplicity, the following function will be given implementing frequency shifting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the following code in this cell is taken and slightly adapted from:\n",
    "# https://gist.github.com/lebedov/4428122\n",
    "\n",
    "import scipy.signal as sig\n",
    "def nextpow2(n):\n",
    "    '''Return the first integer N such that 2**N >= abs(n)'''\n",
    "    return int(np.ceil(np.log2(np.abs(n))))\n",
    "\n",
    "def frequency_shift(signal, fs, shift_amount):\n",
    "    '''\n",
    "    Shift the specified signal by the specified frequency.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : float\n",
    "       input signal to which the effect should be applied\n",
    "    fs : int\n",
    "        sampling frequency in Hz\n",
    "    shift_amount : float\n",
    "       amount of frequency shift (in Hz)\n",
    "   \n",
    "    Return\n",
    "    ----------\n",
    "    signal after application of the frequency shifting effect\n",
    "    \n",
    "    Example use:\n",
    "    -------\n",
    "        signal_frequency_shifted = frequency_shift(audio_in, fs, 100)\n",
    "    '''\n",
    "\n",
    "    # Pad the signal with zeros to prevent the FFT invoked by the transform from\n",
    "    # slowing down the computation:\n",
    "    N_orig = len(signal)\n",
    "    N_padded = 2 ** nextpow2(N_orig)\n",
    "    t = np.arange(0, N_padded)\n",
    "    \n",
    "    # Hilbert transform to get the analytic signal\n",
    "    analytic_signal = sig.hilbert(np.hstack((signal, np.zeros(N_padded - N_orig, signal.dtype))))  # np.hstack concat two signal\n",
    "    frequency_shifted = analytic_signal * np.exp(2j * np.pi * shift_amount * t / fs)\n",
    "    # Extract the real part and truncate to the original length\n",
    "    shifted_frequency_signal = np.real(frequency_shifted[:N_orig])\n",
    "    \n",
    "    return shifted_frequency_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #abe;\">\n",
    "    \n",
    "**Task T7 (Frequency Shifting):**\n",
    "    \n",
    "* Visualise the effect of the frequency shift effect using an appropriate spectral representation.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualise the signal in frequency domain\n",
    "speech_signal_length = len(speech_signal)  # the length of the signal is 928086, we need to find the L_DFT that is the exponential of 2 and should be larger than the length of signal\n",
    "L_DFT = 2**int(np.ceil(np.log2(speech_signal_length)))  #2**20\n",
    "\n",
    "# perform DFT using the FFT algorithm\n",
    "spectrum = np.fft.fft(speech_signal, L_DFT)\n",
    "magnitude_spectrum = np.abs(spectrum)\n",
    "frequency = np.fft.fftfreq(L_DFT, d=1/speech_signal_fs)\n",
    "positive_frequency = frequency[:L_DFT//2]\n",
    "positive_magnitude_spectrum = magnitude_spectrum[:L_DFT//2]\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# plot the full frequency domain of the speech signal\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(frequency, magnitude_spectrum)\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Full frequency domain representation of the signal')\n",
    "\n",
    "# plot the positive frequency domain of the speech signal\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.plot(positive_frequency, positive_magnitude_spectrum)\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Positive frequency domain representation of the signal')\n",
    "\n",
    "# Shift the wave\n",
    "shifted_speech_signal = frequency_shift(signal=speech_signal, fs=speech_signal_fs, shift_amount=2000)\n",
    "shifted_speech_signal_length = len(shifted_speech_signal) # the length of the shifted signal is 928086\n",
    "# we need to find the L_DFT that is the exponential of 2 and should be larger than the length of signal\n",
    "\n",
    "L_DFT_shifted = 2**int(np.ceil(np.log2(shifted_speech_signal_length)))  #2**20\n",
    "# perform DFT using the FFT algorithm\n",
    "spectrum_shifted = np.fft.fft(shifted_speech_signal, L_DFT_shifted)\n",
    "magnitude_spectrum_shifted = np.abs(spectrum_shifted)\n",
    "frequency_shifted = np.fft.fftfreq(L_DFT_shifted, d=1/speech_signal_fs)\n",
    "positive_frequency_shifted = frequency_shifted[:L_DFT_shifted//2]\n",
    "positive_magnitude_spectrum_shifted = magnitude_spectrum_shifted[:L_DFT_shifted//2]\n",
    "\n",
    "# plot the full frequency domain of the shifted speech signal\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.plot(frequency_shifted, magnitude_spectrum_shifted)\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Full frequency domain representation of the shifted signal')\n",
    "\n",
    "# plot the positive frequency domain of the shifted speech signal\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(positive_frequency_shifted, positive_magnitude_spectrum_shifted)\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Positive frequency domain representation of the shifted signal')\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.plot(frequency, magnitude_spectrum, color='r', label='original speech signal')\n",
    "plt.plot(frequency_shifted, magnitude_spectrum_shifted, color='g', label='shifted speech signal')\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Full frequency domain comparation')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "plt.plot(positive_frequency, positive_magnitude_spectrum, color='r', label='original speech signal')\n",
    "plt.plot(positive_frequency_shifted, positive_magnitude_spectrum_shifted, color='g', label='shifted speech signal')\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Positive frequency domain comparation')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(shifted_speech_signal, rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #9696ff;\">\n",
    "    \n",
    "**Question Q4:**\n",
    "\n",
    "* COM3502-4502-6502: Why can the voice be shifted up in frequency much further than\n",
    "it can be shifted down in frequency before it becomes severely distorted? Hint: Calculate a spectrum plot if the answer is not immediately clear to you.\n",
    "* COM4502-6502 ONLY: Your frequency shifter changes all the frequencies present in an input signal. How might it be possible to change the pitch of a voice without altering the formant frequencies?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Question Q4:**\n",
    "\n",
    "<span style=\"font-weight:bold;color:orange\">\n",
    "    We know that the primary frequency range of human speech is about 300 HZ to 3400 HZ, which is the main component of speech. 300 HZ is the lowest effective frequency of speech, and frequencies below this range contribute little to the intelligibility. The audible range of the human ear is 20 HZ to 20000HZ.<br><br>\n",
    "\n",
    "When we shift up the frequency, for example 1000HZ, the new range can be 1300HZ and 4400HZ, this range is still in the audible range of the human ear. At this time, the spectrum structure of the speech signal remains intact, although the overall frequency becomes higher, but the relative spectrum distribution is unchanged, so human can still understand the signal.<br><br>\n",
    "\n",
    "When we shift down the frequency, frequency components will close to 0 HZ or even negative frequencies. However, in the actual signal, there is no negative frequencies, negative frequencies will be folded onto the positive frequency axis, causing aliasing. Downward shift will also cause the original high frequency part to enter the low frequency region, and even lose to the range of human ears (below 20 Hz).<br><br>\n",
    "\n",
    "Therefore, voiced can be shifted up in frequency much further than it can be shifted down in frequency.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harmony Effect\n",
    "\n",
    "A classic ‘robotic’ voice can be achieved by simply adding frequency-shifted speech back to the unprocessed original. This effect is known as ‘harmony’. However, rather than simply adding the signals in equal amounts, we will implement a more general-purpose approach.\n",
    "\n",
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #abe;\">\n",
    "    \n",
    "**Task T8:**\n",
    "    \n",
    "* Implement a function `mixer()` that adds the original speech with the manipulated speech in different proportions. \n",
    "* Implement a function `harmony()` that mixes the input signal with a frequency-shifted version of itself (using the functions `mixer()` and `frequency_shift()`). With your mixer at the $50$-$50$ setting, experiment with different frequency shifts in order to produce the best robotic-sounding output. Report \"your optimal\" setting. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mixer(original_speech_signal, manipulated_speech_signal, percentage_l=0.5):\n",
    "    mixed_speech_signal = percentage_l * original_speech_signal + (1-percentage_l) * manipulated_speech_signal\n",
    "    return mixed_speech_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def harmony(original_speech_signal, fs, shift_amount, percentage_l=0.5):\n",
    "    shifted_speech_signal = frequency_shift(signal=original_speech_signal, fs=fs, shift_amount=shift_amount)\n",
    "    harmony_speech_signal = mixer(original_speech_signal=original_speech_signal, manipulated_speech_signal=shifted_speech_signal, percentage_l=percentage_l)\n",
    "    return harmony_speech_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here to show an example of the harmony effect\n",
    "shift_amount_list = np.arange(0, 800, 100)\n",
    "harmony_speech_signal_list = []\n",
    "for shift in shift_amount_list:\n",
    "    harmony_speech_signal = harmony(original_speech_signal=speech_signal, fs=speech_signal_fs, shift_amount=shift, percentage_l=0.5)\n",
    "    harmony_speech_signal_list.append(harmony_speech_signal)\n",
    "print(harmony_speech_signal_list, len(harmony_speech_signal_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(speech_signal, rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(harmony_speech_signal_list[0], rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(harmony_speech_signal_list[1], rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(harmony_speech_signal_list[2], rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(harmony_speech_signal_list[3], rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(harmony_speech_signal_list[4], rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to question in Task T7:**\n",
    "\n",
    "<span style=\"font-weight:bold;color:orange\">\n",
    "My optimal setting is: a frequency shift of 200 HZ with the mixer set to 50-50 proportion. <br>\n",
    "Because 200HZ frequency shift introduces sufficient harmonic dissonance, producing a clear robotic tone without making the signal too distorted. I determined this optimal setting by systematically listening to the harmony speech signal with frequency shift from 0 to 800 with a footstep 100. <br><br>\n",
    "\n",
    "At 100 HZ, some harmonic dissonance was introduced, but the human speech remained too intelligible.<br>\n",
    "At 200 HZ, the robotic effect was perfect, achieving a balance between clarity and the desired robotic tone.<br>\n",
    "Beyond 200 HZ, the speech signal became increasingly distorted, which were not performed better than 200 HZ.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Frequency Modulation: Vibrato\n",
    "\n",
    "Now that you have the ability to shift the frequencies in a speech signal, it is very easy to implement another common voice manipulation technique - *vibrato*. All that is required is for the frequency shifter to be controlled by the output of an LFO.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #abe;\">\n",
    "    \n",
    "**Task T9:**\n",
    "    \n",
    "* Implement a function `vibrato()` by connecting an LFO to your frequency shifter, and experiment with different values for speed and depth. Note that the LFO output will need to be scaled to provide an appropriate frequency shift range and then added to the output of the frequency shift.\n",
    "    \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def frequency_shift_dynamic(signal, fs, shift_amount):\n",
    "    # Pad the signal with zeros to prevent the FFT invoked by the transform from\n",
    "    # slowing down the computation:\n",
    "    N_orig = len(signal)\n",
    "    N_padded = 2 ** nextpow2(N_orig)\n",
    "    t = np.arange(0, N_padded)\n",
    "    \n",
    "    # Hilbert transform to get the analytic signal\n",
    "    shift_amount = np.pad(shift_amount, (0, N_padded-len(shift_amount)), mode='constant')\n",
    "    analytic_signal = sig.hilbert(np.hstack((signal, np.zeros(N_padded - N_orig, signal.dtype))))  # np.hstack concat two signal\n",
    "    frequency_shifted = analytic_signal * np.exp(2j * np.pi * shift_amount * t / fs)\n",
    "    # Extract the real part and truncate to the original length\n",
    "    vibrato_signal = np.real(frequency_shifted[:N_orig])\n",
    "    \n",
    "    return vibrato_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vibrato effect implementation\n",
    "def vibrato(signal, speed_hz, depth, fs, square_curve=False):\n",
    "    num_samples = len(signal)\n",
    "    lfo_output = lfo_no_scipy(speed_hz=speed_hz, depth=depth, num_samples=num_samples, fs=fs, square_curve=square_curve)[0]\n",
    "    vibrato_signal = frequency_shift_dynamic(signal=signal, fs=fs, shift_amount=lfo_output)\n",
    "\n",
    "    return vibrato_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# experiment with different values for speed and fixed depth\n",
    "speed_list = np.arange(0, 5, 1)\n",
    "vibrato_signal_speed_list = []\n",
    "for speed in speed_list:\n",
    "    vibrato_signal = vibrato(signal=speech_signal, speed_hz=speed, depth=0.5, fs=speech_signal_fs, square_curve=False)\n",
    "    vibrato_signal_speed_list.append(vibrato_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vibrato with depth=0.5, speed_hz=1\n",
    "ipd.Audio(vibrato_signal_speed_list[0], rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vibrato with depth=0.5, speed_hz=2\n",
    "ipd.Audio(vibrato_signal_speed_list[1], rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vibrato with depth=0.5, speed_hz=3\n",
    "ipd.Audio(vibrato_signal_speed_list[2], rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vibrato with depth=0.5, speed_hz=4\n",
    "ipd.Audio(vibrato_signal_speed_list[3], rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# experiment with different values for depth and fixed speed\n",
    "depth_list = np.arange(0, 1, 0.2)\n",
    "vibrato_signal_depth_list = []\n",
    "for depth in depth_list:\n",
    "    vibrato_signal = vibrato(signal=speech_signal, speed_hz=3, depth=depth, fs=speech_signal_fs, square_curve=False)\n",
    "    vibrato_signal_depth_list.append(vibrato_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vibrato with speed_hz=3, depth=0.2\n",
    "ipd.Audio(vibrato_signal_depth_list[1], rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vibrato with speed_hz=3, depth=0.4\n",
    "ipd.Audio(vibrato_signal_depth_list[2], rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vibrato with speed_hz=3, depth=0.6\n",
    "ipd.Audio(vibrato_signal_depth_list[3], rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vibrato with speed_hz=3, depth=0.8\n",
    "ipd.Audio(vibrato_signal_depth_list[4], rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a sine wave to implement vibrato effect\n",
    "sine_wave_fs = 8000\n",
    "sine_wave_time = 10\n",
    "sine_wave_t = np.arange(0, sine_wave_time, 1 / sine_wave_fs)\n",
    "sine_wave = np.sin(2 * np.pi * 50 * sine_wave_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vibrato_sine = vibrato(signal=sine_wave, speed_hz=1, depth=0.1, fs=8000, square_curve=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(sine_wave, rate=sine_wave_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(vibrato_sine, rate=sine_wave_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Delay Effect - Echo and Comb Filter\n",
    "\n",
    "Many interesting voice FX can be achieved by delaying the signal and recombining it with itself. \n",
    "\n",
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #abe;\">\n",
    "    \n",
    "**Task T10:**\n",
    "    \n",
    "* Implement a function `echo()` which mixes a signal $s(t)$ with itself in a delayed version, i.e. $s(t-t_0)$. Experiment with various values for the delay $t_0$, and note the different effects you can achieve with delays \n",
    "  * below $20$ msecs\n",
    "  * between $20$ and $100$ msecs, and \n",
    "  * above $100$ msecs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def echo(signal, fs, delay_ms, percentage=0.7):\n",
    "    delay_samples = int(delay_ms * fs / 1000)  \n",
    "    echoed_signal = np.zeros(len(signal) + delay_samples)  # Initialize echoed signal with zeros\n",
    "    echoed_signal[:len(signal)] += signal  # Add original signal\n",
    "    echoed_signal[delay_samples:] += percentage * signal\n",
    "    total_samples = len(echoed_signal)\n",
    "    duration = total_samples/fs\n",
    "    t_echoed_signal = np.linspace(0, duration, len(echoed_signal), endpoint=False)\n",
    "    return t_echoed_signal, echoed_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here to show an example of the echo effect\n",
    "speech_signal_time = len(speech_signal)/speech_signal_fs\n",
    "t = np.linspace(0, speech_signal_time, len(speech_signal), endpoint=False)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(t, speech_signal)\n",
    "plt.xlabel('Time in second')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Time domain representation of the signal')\n",
    "plt.grid(True)\n",
    "\n",
    "delay_list = [10, 15, 50, 80, 150, 250]\n",
    "num_delay = len(delay_list)\n",
    "echoed_signal_list = []\n",
    "plt.figure(figsize=(25, 6 * num_delay))\n",
    "for i, delay in enumerate(delay_list):\n",
    "    t_echoed_signal, echoed_signal = echo(signal=speech_signal, fs=speech_signal_fs, delay_ms=delay, percentage=0.7)\n",
    "    echoed_signal_list.append(echoed_signal)\n",
    "    plt.subplot(num_delay, 3, i*3 + 1)\n",
    "    plt.plot(t_echoed_signal, echoed_signal)\n",
    "    plt.xlabel('Time In Second')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title(f\"Time domain (Delay={delay} ms)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # create the frequency domain representation of the signal\n",
    "    echoed_signal_fs = speech_signal_fs\n",
    "    echoed_signal_length = len(echoed_signal)\n",
    "    L_DFT = 2**int(np.ceil(np.log2(echoed_signal_length)))\n",
    "    freq_response = np.fft.fft(echoed_signal, L_DFT)\n",
    "    magnitude = np.abs(freq_response)\n",
    "    frequencies = np.fft.fftfreq(L_DFT, 1 / echoed_signal_fs)\n",
    "    positive_frequencies = frequencies[:len(frequencies)//2]\n",
    "    positive_magnitude = magnitude[:len(freq_response)//2]\n",
    "    plt.subplot(num_delay, 3, i*3 + 2)\n",
    "    plt.plot(positive_frequencies, positive_magnitude)\n",
    "    plt.title(f\"Frequency domain (Delay={delay} ms)\")\n",
    "    plt.xlabel(\"Frequency in HZ\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(num_delay, 3, i*3 + 3)\n",
    "    plt.specgram(echoed_signal, Fs=echoed_signal_fs, NFFT=1024, noverlap=512, cmap='viridis', scale='dB')\n",
    "    plt.title(f\"Spectrogram (Delay={delay} ms)\")\n",
    "    plt.colorbar(label='dB')\n",
    "    plt.xlabel('Time in seconds')\n",
    "    plt.ylabel('Frequency in HZ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the original speech signal\n",
    "ipd.Audio(speech_signal, rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the echoed speech signal with delay 10ms\n",
    "ipd.Audio(echoed_signal_list[0], rate=echoed_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the echoed speech signal with delay 15ms\n",
    "ipd.Audio(echoed_signal_list[1], rate=echoed_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the echoed speech signal with delay 50ms\n",
    "ipd.Audio(echoed_signal_list[2], rate=echoed_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the echoed speech signal with delay 80ms\n",
    "ipd.Audio(echoed_signal_list[3], rate=echoed_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the echoed speech signal with delay 150ms\n",
    "ipd.Audio(echoed_signal_list[4], rate=echoed_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the echoed speech signal with delay 250ms\n",
    "ipd.Audio(echoed_signal_list[5], rate=echoed_signal_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comb Filtering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should observe that with delays below $20$ msec in your function `echo()`, the signals combine to create a subtle ‘phasing’ effect. This is known as ‘comb filtering’ as the signal is effectively interfering with itself, and frequency components corresponding to multiples of the delay time are enhanced or cancelled out (due to ‘superposition’). Delays between $20$ and $100$ msecs give the effect of the voice being in a reverberant room. Delays above $100$ msecs sound like distant echoes.\n",
    "\n",
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #abe;\">\n",
    "    \n",
    "**Task T11:**\n",
    "    \n",
    "* Visualise the impulse response of your function `echo()` as well as the transfer function. Can you give an explanation from having a look at the transfer function, why this effect would be called *comb filter*?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an impulse signal\n",
    "length = 1000\n",
    "impulse_position = 0\n",
    "impulse_signal = np.zeros(length)\n",
    "impulse_signal[impulse_position] = 1\n",
    "impulse_signal_fs = 8000\n",
    "\n",
    "# echo effect\n",
    "plt.figure(figsize=(20, 6 * num_delay))\n",
    "for i, delay in enumerate(delay_list):\n",
    "    plt.subplot(num_delay, 2, i * 2+1)\n",
    "    t_echoed_signal, impulse_response = echo(signal=impulse_signal, fs=impulse_signal_fs, delay_ms=delay, percentage=0.7)\n",
    "    plt.plot(t_echoed_signal, impulse_response, label='Impulse Response')\n",
    "    plt.title(f\"Impulse Response of the Echo Function with delay {delay}ms\")\n",
    "    plt.xlabel(\"Time in seconds\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid()\n",
    "    \n",
    "    # Impulse response of the Echo Function\n",
    "    impulse_response_length = len(impulse_response)\n",
    "    L_DFT = 2**int(np.ceil(np.log2(impulse_response_length)))\n",
    "    freq_response = np.fft.fft(impulse_response, L_DFT)\n",
    "    frequencies = np.fft.fftfreq(L_DFT, 1 / impulse_signal_fs)\n",
    "    magnitude = np.abs(freq_response)\n",
    "    positive_magnitude = magnitude[:len(freq_response)//2]\n",
    "    positive_frequencies = frequencies[:len(freq_response)//2]\n",
    "    plt.subplot(num_delay, 2, i * 2 + 2)\n",
    "    plt.plot(positive_frequencies, positive_magnitude)\n",
    "    plt.title(f\"Magnitude Response of the Transfer Function with delay {delay}ms\")\n",
    "    plt.xlabel(\"Frequency in HZ\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to question in Task T11:**\n",
    "\n",
    "<span style=\"font-weight:bold;color:orange\">\n",
    "The effect is called a comb filter because transfer function's magnitude response exhibits regularly spaced peaks and valleys across the frequency axis, the shape of these transfer function resembling the shape of comb.The peaks in the transfer function occur at frequencies where the original signal and delayed signal align in phase. This constructive interference amplifies specific frequency components. The valleys in the transfer function occur at frequencies where the original signal and the delayed signal are out of phase. This resulting in destructive interference and complete cancellation of those frequencies. <br><br>\n",
    "\n",
    "The interference pattern (constructive and destructive) repeats periodically based on the delay time. As seen in transfer function graphs above, the evenly spaced peaks and valleys along the frequency domain create a comb-like shape.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flanger \n",
    "\n",
    "It is possible to use an LFO to vary the delay. The resulting effect is known as a *flanger*.\n",
    "\n",
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #abe;\">\n",
    "    \n",
    "**Task T12:**\n",
    "    \n",
    "* Add an LFO to your ‘delay’ to create a ‘flanger’, and experiment with different settings. Note that you will need to scale the output of the LFO, and you will get different effects depending on whether the delayed signal is mixed with the original or not.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flanger(signal, max_delay_ms, fs, speed_hz, depth, mix_ratio, square_curve=True):\n",
    "    num_samples = len(signal)\n",
    "    lfo_output = lfo_no_scipy(speed_hz=speed_hz, depth=depth, num_samples=num_samples, fs=fs, square_curve=square_curve)[0]\n",
    "    # negative delay is not meaningful in physics, therefore we scale the lfo output from (-depth, depth) to (0, 1)\n",
    "    scaled_lfo_output = (lfo_output + depth)/(2 * depth)\n",
    "    max_delay_samples = int(max_delay_ms / 1000 * fs)\n",
    "    delay_samples = (scaled_lfo_output * max_delay_samples).astype(int)\n",
    "    \n",
    "    # create the flanger signal\n",
    "    flanger_signal = np.zeros_like(signal)\n",
    "    for n in range(num_samples):\n",
    "        delay = delay_samples[n]\n",
    "        if n - delay >= 0:\n",
    "            flanger_signal[n] = signal[n] * (1 - mix_ratio) + signal[n - delay] * mix_ratio\n",
    "        else:\n",
    "            flanger_signal[n] = signal[n]  # Handle out-of-bounds delay\n",
    "    return flanger_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mix_ratio_list = np.arange(0, 1.2, 0.2)\n",
    "flanger_signal_list = []\n",
    "for mix_ratio in mix_ratio_list:\n",
    "    flanger_signal = flanger(signal=speech_signal, max_delay_ms=15, fs=speech_signal_fs, speed_hz=10, depth=1, mix_ratio=mix_ratio, square_curve=False)\n",
    "    flanger_signal_list.append(flanger_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, flanger_signal in enumerate(flanger_signal_list):\n",
    "    plt.figure(figsize=(16, 6*len(flanger_signal_list)))\n",
    "    plt.subplot(len(flanger_signal_list), 2, i*2+1)\n",
    "    flanger_signal_length = len(flanger_signal)\n",
    "    L_DFT = 2**int(np.ceil(np.log2(flanger_signal_length)))\n",
    "    freq_response = np.fft.fft(flanger_signal, L_DFT)\n",
    "    frequencies = np.fft.fftfreq(L_DFT, 1 / speech_signal_fs)\n",
    "    magnitude = np.abs(freq_response)\n",
    "    positive_magnitude = magnitude[:len(freq_response)//2]\n",
    "    positive_frequencies = frequencies[:len(freq_response)//2]\n",
    "    plt.plot(positive_frequencies, positive_magnitude)\n",
    "    plt.title(f'Frequency representation of the Flanger signal with mix ratio={((i+1)*0.2):.2f}')\n",
    "    plt.xlabel(\"Frequency in HZ\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(len(flanger_signal_list), 2, i*2+2)\n",
    "    plt.specgram(flanger_signal, Fs=speech_signal_fs, NFFT=1024, noverlap=512, cmap='viridis', scale='dB')\n",
    "    plt.title(f\"Spectrogram of the Flanger signal with mix ratio={((i+1)*0.2):.2f}\")\n",
    "    plt.colorbar(label='dB')\n",
    "    plt.xlabel('Time in seconds')\n",
    "    plt.ylabel('Frequency in HZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# flanger signal with mix ratio=0.2\n",
    "ipd.Audio(flanger_signal_list[0], rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# flanger signal with mix ratio=0.4\n",
    "ipd.Audio(flanger_signal_list[1], rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# flanger signal with mix ratio=0.6\n",
    "ipd.Audio(flanger_signal_list[2], rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# flanger signal with mix ratio=0.8\n",
    "ipd.Audio(flanger_signal_list[3], rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# flanger signal with mix ratio=1.0\n",
    "ipd.Audio(flanger_signal_list[4], rate=speech_signal_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Frequency Analysis\n",
    "\n",
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #9696ff;\">\n",
    "    \n",
    "**Question Q5:**\n",
    "\n",
    "* COM3502-4502-6502: \n",
    "    * What does FFT stand for and what does an FFT do?\n",
    "* COM4502-6502 ONLY: What is a DFT and how is it different from an FFT?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Question Q5:**\n",
    "\n",
    "<span style=\"font-weight:bold;color:orange\">\n",
    "FFT stands for Fast Fourier Transform. It is an efficient algorithm for computing the Discrete Fourier Transform (DFT) of a signal, which transforms a time-domain representation into its frequency-domain representation. To accelerate the computation, FFT leverages the structure of the DFT when the signal length is a power of 2. If the signal length is not a power of 2, zero-padding is applied to extend it to the nearest power of 2. This optimization reduces the computational complexity from  $O(N^2)$  for a regular DFT to  $O(NlogN)$ , making the transformation process significantly faster.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Spectrogram Step-by-Step (Task T13 for COM4502-6502 only, Task T14 for all students)\n",
    "\n",
    "The magnitude $|X[n, \\ell]|$ of the STFT for all $n$ and $\\ell$ is known as the [spectrogram](https://en.wikipedia.org/wiki/Spectrogram) of a signal. It is frequently used to analyse signals in the time-frequency domain, for instance by a [spectrum analyser](https://en.wikipedia.org/wiki/Spectrum_analyzer). It can be interpreted as a *image* of the signal with (block) time direction on the $x$ axis and (discrete) frequency $n$ on the y axis.\n",
    "\n",
    "From Lab Sheet 3 we already know how to breack a long signal into block, a.k.a. frames.\n",
    "\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #abe;\">\n",
    "    \n",
    "**Task 13: Manual Spectrogram Calculation (for COM4502-6502 only)**\n",
    "    \n",
    "<ul>\n",
    "    <li> \n",
    "        Implement a function <code>calc_SpectralPoint(xk,n)</code> which calculates a spectral point for one discrete frequency $n$ from a input frame $x[k]$, i.e. a function which implements the well-known DFT equation\n",
    "    $$\n",
    "    \\mathrm{DFT}\\{x[k]\\}   =  X[n] = \\frac{1}{L_{\\mathrm{DFT}}} \\sum \\limits_{k=0}^{L_{\\mathrm{DFT}}-1} x[k]  e^{j 2 \\pi k n /L_{\\mathrm{DFT}}}\n",
    "    $$ \n",
    "    for one fixed $n$.\n",
    "    </li>\n",
    "    <li> \n",
    "        Implement a very similar function <code>calc_SpectralPointWindowed(xk,n)</code> which calculates a spectral point for one discrete frequency $n$ from a input frame $x[k]$, but in addition applies a window function $w[k]$ to the frame $x[k]$, i.e. the function should calculate\n",
    "    $$\n",
    "    \\mathrm{DFT}\\{w[k] \\cdot x[k]\\}   =  X^{\\mathrm{w}}[n] = \\frac{1}{L_{\\mathrm{DFT}}} \\sum \\limits_{k=0}^{L_{\\mathrm{DFT}}-1} w[k] x[k]  e^{j 2 \\pi k n /L_{\\mathrm{DFT}}}\n",
    "    $$ \n",
    "    for one fixed $n$. The window should have the same length $L_{\\mathrm{DFT}}$ as your frame and should be one of the windows we discussed during the lecture.\n",
    "    </li>\n",
    "    <li> \n",
    "        The functions above only calculate one spectral value at a time. To obtain a full spectrum, implement a function <code>calc_Manitude_Spectrum()</code> which transforms every windowed frame to the frequency domain and calculates all positive frequencies, i.e. for $0 \\leq n \\leq L_{\\mathrm{DFT}}/2+1$.\n",
    "    </li>\n",
    "    <li> \n",
    "        Create a function <code>create_spectrogram()</code>, which splits the complete input sequence (e.g. a loaded WAVE file) into blocks of length $L_{\\mathrm{DFT}}$. These may be overlapping. For each block the spectrum should be calculated using the previously implemented function <code>calc_Manitude_Spectrum()</code> and all spectra should be collected to form a spectrogram (e.g. as columns of a matrix).\n",
    "    </li>\n",
    "    <li> \n",
    "        Concatenate the resulting spectra to a spectrogram and display the resulting spectrogram. You can use <code>matplotlib</code>'s <code><a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\">imshow()</a></code> function for manually plotting the spectrogram image. Note that a spectrogram is usually shown in dB scaling.\n",
    "    </li>\n",
    "    <li>\n",
    "        Visualise the input signal $x[k]$ as a spectrogram for (i) a speech signal and (ii) for a chirp/sweep signal. \n",
    "    </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the  DFT equation\n",
    "    $$\\mathrm{DFT}\\{x[k]\\}   =  X[n] = \\frac{1}{L_{\\mathrm{DFT}}} \\sum \\limits_{k=0}^{L_{\\mathrm{DFT}}-1} x[k]  e^{j 2 \\pi k n /L_{\\mathrm{DFT}}}$$ for one fixed $n$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_SpectralPoint(xk,n):\n",
    "    '''\n",
    "    Implementation of the Discrete Fourier Transform (DFT).\n",
    "    Calculates the Fourier coefficient X[n] for one discrete frequency n \n",
    "    \n",
    "    Input: \n",
    "    xk:     time domain signal vector\n",
    "    n:      discrete frequency to be calculated\n",
    "    \n",
    "    Output \n",
    "    Xn : discrete frequency domain point for frequency n\n",
    "    '''\n",
    "    \n",
    "    # Your code here\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement DFT of windowed frame\n",
    "$$\\mathrm{DFT}\\{w[k] \\cdot x[k]\\}   =  X^{\\mathrm{w}}[n]  = \\frac{1}{L_{\\mathrm{DFT}}} \\sum \\limits_{k=0}^{L_{\\mathrm{DFT}}-1} w[k] x[k]  e^{j 2 \\pi k n /L_{\\mathrm{DFT}}}$$ for one fixed $n$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_SpectralPointWindowed(xk,n,window=False):\n",
    "    '''\n",
    "    Implementation of the Discrete Fourier Transform (DFT).\n",
    "    Calculates the Fourier coefficient X[n] for one discrete frequency n \n",
    "    \n",
    "    Input: \n",
    "    xk:     time domain signal vector\n",
    "    n:      discrete frequency to be calculated\n",
    "    window: (optional): can be False (no window) or a window name from   \n",
    "            the list of available numpy window functions, e.g. \n",
    "            np.hamming, np.bartlett, np.blackman, np.hanning, np.kaiser\n",
    "            type: function\n",
    "            (feel free to implement the window differently)\n",
    "    \n",
    "    Output \n",
    "    Xn : dicrete frequency domain point for frequency n\n",
    "    '''\n",
    "    # Your code here\n",
    "    # L_DFT = ???\n",
    "    # ...\n",
    "    \n",
    "    if window == False:\n",
    "        win = np.ones(L_DFT) # this is a window with no effect\n",
    "    else:\n",
    "        None # replace this by your own window\n",
    "        \n",
    "    # Your code here\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function <code>calc_Manitude_Spectrum()</code> is supposed to transform every (windowed or not windowed) frame to the frequency domain and to calculate all positive frequencies, i.e. $X[n]$ or $X^{\\mathrm{w}}[n]$ for $0 \\leq n \\leq L_{\\mathrm{DFT}}/2+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_Manitude_Spectrum(xk):\n",
    "    '''\n",
    "    Compute Fourier coefficients up to the Nyquest Limit (fs/2), i.e. Xn for n=0,...,L_DFT/2 \n",
    "    using one of the two functions created before.\n",
    "    and multiply the absolute value of the Fourier coefficients by 2, \n",
    "    to account for the symmetry of the Fourier coefficients above the Nyquest Limit. \n",
    "    '''\n",
    "    # Your code here\n",
    "    # ...\n",
    "    \n",
    "    # probably there should be a loop over n here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function <code>create_spectrogram()</code> should calculate all spectra needed for your spectrogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram(x, L_DFT=512, noverlap):\n",
    "    '''\n",
    "           x: original time series\n",
    "       L_DFT: The number of data points used in each block for the DFT. The default value is 512. \n",
    "    noverlap: The number of points of overlap between blocks. The default value is 256. \n",
    "    '''\n",
    "    # Your code here\n",
    "    # ...\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function can be used to actually display the created spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram( #..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code actually calculates and plots your spectrogram (for the two signals mentioned above). Feel free to adapt parameters `L_DFT` and `noverlap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load or create signal\n",
    "\n",
    "# create and plot spectrogram (generated using your functions above)\n",
    "L_DFT    = 256 # DFT length\n",
    "noverlap = 84  # number of overlapping samples\n",
    "starts, spec = create_spectrogram( #...\n",
    "plot_spectrogram(#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #999; padding: 10px; background: #abe;\">\n",
    "    \n",
    "**Task 14: Apply Short-Time Fourier Transform (STFT)**\n",
    "    \n",
    "<ul>\n",
    "    <li> \n",
    "        Explain the purpose of the Short-Time Fourier Transform (STFT) and how it differs from a regular Fourier Transform. Provide a brief(!) explanation below.\n",
    "    </li>\n",
    "    <li> \n",
    "        Apply the STFT to the signal generated in Task T2 using a window size of $256$ samples with $50$% overlap (COM4502-6502 students should use the previously created code (if Task T13 was completed), existing functions can be used by COM3502 students (or if COM4502-6502 students did not complete Task T13).\n",
    "    </li>\n",
    "    <li> \n",
    "        Interpret the spectrogram. What information does it provide about the signal? Briefly(!) explain what you see in the spectrogram and how it represents the frequency content over time.\n",
    "    </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.signal import stft\n",
    "synthetic_signal_fs = 8000\n",
    "window_size = 256\n",
    "overlap = 256 * 0.5\n",
    "frequencies, times, Zxx = stft(audio_signal, audio_signal_fs, nperseg=window_size, noverlap=overlap)\n",
    "magnitude_dB = 20 * np.log10(np.abs(Zxx))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pcolormesh(times, frequencies, magnitude_dB, shading='gouraud')\n",
    "plt.title(\"Spectrogram of the Synthetic Signal (STFT)\")\n",
    "plt.ylabel(\"Frequency in HZ\")\n",
    "plt.xlabel(\"Time in seconds\")\n",
    "plt.colorbar(label='Magnitude (dB)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to question in Task T14:**\n",
    "\n",
    "<span style=\"font-weight:bold;color:orange\">\n",
    "T14. First Question<br><br>\n",
    "The Short-Time Fourier Transform analyzes a non-stationary signal's frequency content over time by dividing it into short frames (where the signal can be approximated as stationary) and applying a window function (it can be Hanning or Hamming window) to smooth edges of each frame. The Fourier Transform is then applied to each windowed frame, providing a time-frequency representation of the signal.<br><br>\n",
    "\n",
    "In contrast, a regular Fourier Transform provides only the frequency content of the entire signal, assuming the signal is stationary, lacks time information. STFT enables tracking frequency changes over time, which is crucial for non-stationary signals analysis.<br><br>\n",
    "    \n",
    "T14. Second Question<br><br>\n",
    "The spectrogram presents the frequency content of the synthetic signal over time. There are three distinct, constant horizontal bands centered at 100 HZ, 300 HZ, and 500 HZ, which correspond to the three sinusoidal frequencies that we construct the synthetic signal. The absence of variation in the bands over time reflects the stationary nature of the signal, as the frequencies and phases remain unchanged throughout the 2-second duration. This indicates that the synthetic signal consists of three pure sinusoids with no noise or modulation.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitch analysis\n",
    "\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #abe;\">\n",
    "    \n",
    "**Task 15: Pitch analysis**\n",
    "    \n",
    "<ul>\n",
    "    <li> \n",
    "         Extract and plot the pitch contour of a speech signal over time. Write code to estimate the pitch for each frame using an autocorrelation-based pitch detection algorithm. Briefly discuss the variations in pitch. What can pitch tell us about the speaker’s speech patterns? Provide a brief analysis of the pitch contour.\n",
    "    </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the speech signal into frames\n",
    "speech_signal_length = len(speech_signal)   # there are 928086 samples in the speech signal\n",
    "frame_duration = 0.03\n",
    "frame_length = int(frame_duration * speech_signal_fs)\n",
    "overlap_factor = 2   # 50% overlap between frames\n",
    "overlap_length = int(np.round(frame_length/overlap_factor))\n",
    "num_frames = (speech_signal_length - overlap_length) // overlap_length\n",
    "\n",
    "# create the frame list\n",
    "frames_list = []\n",
    "start_sample = 0\n",
    "for i in range(num_frames):\n",
    "    end_sample = start_sample + frame_length\n",
    "    sample_vec = speech_signal[start_sample:end_sample]\n",
    "    start_sample += overlap_length\n",
    "    frames_list.append(sample_vec)\n",
    "print(len(frames_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pitch_list = []\n",
    "freq_min = 50  # lower frequency in HZ defining our search range\n",
    "freq_max = 500  # upper frequency in HZ defining our search range\n",
    "for frame_signal in frames_list:\n",
    "    frame_signal_normalized = (frame_signal - np.mean(frame_signal)) / np.max(np.abs(frame_signal))\n",
    "    acf = 1/len(frame_signal_normalized)*np.correlate(frame_signal_normalized, frame_signal_normalized, mode='full')\n",
    "    L = len(frame_signal_normalized)\n",
    "    kappa = np.arange(-(L-1), L)\n",
    "    kappa_acf_center = len(acf)//2\n",
    "    kappa_min = kappa_acf_center + int(np.round(fs / freq_max))\n",
    "    kappa_max = kappa_acf_center + int(np.round(fs / freq_min))\n",
    "    max_correlation_kappa = kappa_min + np.argmax(acf[kappa_min : kappa_max + 1])\n",
    "    # calculate pitch frequency\n",
    "    pitch = fs/(max_correlation_kappa-kappa_acf_center)\n",
    "    pitch_list.append(pitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_time = ((len(frames_list) - 1) * overlap_length + frame_length) / speech_signal_fs\n",
    "time = np.linspace(0, total_time, num_frames)\n",
    "plt.plot(time, pitch_list)\n",
    "plt.xlabel('Time in Seconds')\n",
    "plt.ylabel('Frequency in HZ')\n",
    "plt.title('Pitch Contour of the speech signal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to question in Task T15:**\n",
    "\n",
    "<span style=\"font-weight:bold;color:orange\">\n",
    "From the pitch contour diagram, we observe distinct variations in pitch over time:<br>\n",
    "1. The pitch contour shows significant oscillations, indicating the speaker use varying intonations. These fluctuations may correspond to changes in emotion within the speech.<br><br>\n",
    "2. Higher pitch values may represent more excited intonations, whereas lower pitch values likely correspond to calmer statements.<br><br>\n",
    "3. Some regions approach zero or exhibit abrupt interruptions, likely corresponding to pauses or unvoiced segments in the speech.<br><br>\n",
    "\n",
    "The variations in the pitch contour reflect the speaker's dynamic use of tone and rhythm in speech. These fluctuations provide insights into the speaker's emotions and sentence structures. For example, rising pitch may indicate a question, while a clamer pitch may indicate a statement. Furthermore, pitch also reveals individual vocal characteristics, such as generally lower pitch for male voices compared to female voices.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Synthesis \n",
    "\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #abe;\">\n",
    "    \n",
    "**Task 16: Synthesize a simple speech sound**\n",
    "    \n",
    "<ul>\n",
    "    <li> \n",
    "        Generate a vowel sound (e.g., “ah”) using a source-filter model, where a glottal pulse train with a frequency of 120 Hz is filtered by a vocal tract filter.\n",
    "        Implement the glottal pulse train as a periodic signal and apply a simple formant-based filter. Plot the resulting waveform.\n",
    "        <li> \n",
    "            Hint: Use the following formant frequencies <code>formant_freqs = [730, 1090, 2440]</code> to create filters with bandwiths of <code>bandwidths = [80, 90, 120]</code>.\n",
    "        </li>\n",
    "    </li>\n",
    "    <li> \n",
    "        Plot the spectrogram of the synthesized sound. Compare it to the spectrogram of a real speech sample (which you can record yourself) containing the same vowel sound.\n",
    "        Discuss any differences observed between synthetic and real speech.\n",
    "    </li>\n",
    "    <li> \n",
    "        Briefly(!) discuss any differences observed between synthetic and real speech.\n",
    "    </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create excitation signal function\n",
    "def excitation_signal(excitation_period, length, offset=0, noise_excitation_percentage=0):\n",
    "    impulse = np.zeros(length)\n",
    "    impulse[offset::excitation_period] = 1  # Set impulse train\n",
    "    noise = np.random.normal(size=length)\n",
    "    # Normalize noise energy to match the impulse energy\n",
    "    noise *= rms(impulse) / rms(noise)\n",
    "    # Combine impulse and noise signals\n",
    "    return (1 - noise_excitation_percentage) * impulse + noise_excitation_percentage * noise\n",
    "\n",
    "# RMS (Root Mean Square) function\n",
    "def rms(signal):\n",
    "    return np.sqrt(np.mean(signal**2))\n",
    "\n",
    "def formant_filter(signal_in, sample_rate, formant_freqs, bandwidths):\n",
    "    signal_out = signal_in.copy()\n",
    "    nyquist = sample_rate / 2\n",
    "    for f_c, bw in zip(formant_freqs, bandwidths):\n",
    "        # Normalize frequencies to Nyquist frequency\n",
    "        low = (f_c - bw / 2) / nyquist  # Low cut-off normalized\n",
    "        high = (f_c + bw / 2) / nyquist # High cut-off normalized\n",
    "        \n",
    "        # Design a Butterworth bandpass filter\n",
    "        b, a = signal.butter(2, [low, high], btype='band')\n",
    "        # Apply the filter to the signal\n",
    "        signal_out = signal.lfilter(b, a, signal_out)\n",
    "    signal_out = signal_out / np.max(np.abs(signal_out)) \n",
    "    return signal_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "synthesis_signal_fs = 44100\n",
    "duration = 1\n",
    "length = int(synthesis_signal_fs * duration)\n",
    "f0 = 120\n",
    "excitation_period = synthesis_signal_fs // f0\n",
    "noise_percentage = 0.1\n",
    "\n",
    "# Generate excitation signal\n",
    "excitation = excitation_signal(excitation_period, length, noise_excitation_percentage=noise_percentage)\n",
    "formant_freqs = [730, 1090, 2440]\n",
    "bandwidths = [80, 90, 120]\n",
    "synthesis_signal = formant_filter(excitation, synthesis_signal_fs, formant_freqs, bandwidths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualise the synthesis signal in time domain\n",
    "synthesis_signal_time = len(synthesis_signal)/synthesis_signal_fs\n",
    "synthesis_signal_t = np.linspace(0, synthesis_signal_time, len(synthesis_signal), endpoint=False)\n",
    "plt.figure(figsize=(12,6*3))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(synthesis_signal_t, synthesis_signal)\n",
    "plt.xlabel('Time in second')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Time domain representation of the synthesis signal')\n",
    "\n",
    "# Visualise the synthesis signal in frequency domain\n",
    "synthesis_signal_length = len(synthesis_signal)  \n",
    "# we need to find the L_DFT that is the exponential of 2 and should be larger than the length of signal\n",
    "L_DFT = 2**int(np.ceil(np.log2(synthesis_signal_length)))  #2**20\n",
    "\n",
    "# perform DFT using the FFT algorithm\n",
    "spectrum = np.fft.fft(synthesis_signal, L_DFT)\n",
    "magnitude_spectrum = np.abs(spectrum)\n",
    "frequency = np.fft.fftfreq(L_DFT, d=1/synthesis_signal_fs) # the order of frequency is correspond to the output of FFT\n",
    "positive_frequency = frequency[:L_DFT//2]\n",
    "positive_magnitude_spectrum = magnitude_spectrum[:L_DFT//2]\n",
    "\n",
    "# plot the positive frequency domain of the speech signal\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(positive_frequency, positive_magnitude_spectrum)\n",
    "plt.xlabel('Frequency in HZ')\n",
    "# plt.xlim(500, 1200)\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Positive frequency domain representation of the synthesis signal')\n",
    "\n",
    "# Visualise the synthesis signal in time-frequency \n",
    "plt.subplot(3, 1, 3)\n",
    "plt.specgram(synthesis_signal, Fs=synthesis_signal_fs, NFFT=512, noverlap=256, cmap='viridis', scale='dB')\n",
    "plt.title('spectrogram with DFT length of 512')\n",
    "plt.colorbar(label='dB')\n",
    "plt.xlabel('Time in seconds')\n",
    "plt.ylabel('Frequency in HZ')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(synthesis_signal, rate=synthesis_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read my record ah signal\n",
    "ah_recording_file = 'ah_recording.wav'\n",
    "ah_signal, ah_signal_fs = sf.read(ah_recording_file)\n",
    "print(ah_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(ah_signal, rate=ah_signal_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualise the ah_recording signal in time domain\n",
    "ah_signal_time = len(ah_signal)/ah_signal_fs\n",
    "ah_signal_t = np.linspace(0, ah_signal_time, len(ah_signal), endpoint=False)\n",
    "plt.figure(figsize=(12,6*3))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(ah_signal_t, ah_signal)\n",
    "plt.xlabel('Time in second')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Time domain representation of the ah signal')\n",
    "\n",
    "# Visualise the signal in frequency domain\n",
    "ah_signal_length = len(ah_signal)  \n",
    "# we need to find the L_DFT that is the exponential of 2 and should be larger than the length of signal\n",
    "L_DFT = 2**int(np.ceil(np.log2(ah_signal_length)))  #2**20\n",
    "\n",
    "# perform DFT using the FFT algorithm\n",
    "spectrum = np.fft.fft(ah_signal, L_DFT)\n",
    "magnitude_spectrum = np.abs(spectrum)\n",
    "frequency = np.fft.fftfreq(L_DFT, d=1/ah_signal_fs) # the order of frequency is correspond to the output of FFT\n",
    "positive_frequency = frequency[:L_DFT//2]\n",
    "positive_magnitude_spectrum = magnitude_spectrum[:L_DFT//2]\n",
    "\n",
    "# plot the positive frequency domain of the speech signal\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(positive_frequency, positive_magnitude_spectrum)\n",
    "plt.xlabel('Frequency in HZ')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Positive frequency domain representation of the ah signal')\n",
    "\n",
    "# Visualise the speech signal in time-frequency \n",
    "plt.subplot(3, 1, 3)\n",
    "plt.specgram(ah_signal, Fs=ah_signal_fs, NFFT=512, noverlap=256, cmap='viridis', scale='dB')\n",
    "plt.title('spectrogram with DFT length of 512')\n",
    "plt.colorbar(label='dB')\n",
    "plt.xlabel('Time in seconds')\n",
    "plt.ylabel('Frequency in HZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to question in Task T16:**\n",
    "\n",
    "<span style=\"font-weight:bold;color:orange\">\n",
    "   The synthesized spectrogram displays clear and static formant frequencies centered at 730 Hz, 1090 Hz, and 2440 Hz, with bandwidth deviations of 80, 90, and 120 Hz, respectively. The evenly spaced harmonics arise from the idealized glottal pulse and formant filtering process. In contrast, the real speech spectrogram reveals dynamic formant variations and broader energy distribution, particularly in the 0–5000 Hz range, with a prominent peak at the fundamental frequency of 120 Hz. The formant frequencies in real speech fluctuate slightly, observed around 700–800 Hz and 1000–1200 Hz, but the third formant appears lower, between 1900–2100 Hz, compared to the synthesized 2440 Hz. This difference may stem from slight deviations in my pronunciation of the \"ah\" sound. Additionally, the real speech spectrogram exhibits irregular energy above 3000 Hz, reflecting the natural articulation process and complex dynamics of vocal tract movement.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equaliser (for COM4502-6502 only)\n",
    "\n",
    "We want to design an equaliser like shown in the picture below as a hardware system.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Yamaha_EQ-500_Graphic_Equalizer.jpg/1920px-Yamaha_EQ-500_Graphic_Equalizer.jpg\" align=\"center\" style=\"width: 500px;\"/>\n",
    "<center><span style=\"font-size:smaller\">\n",
    "    Picture taken from <a href=\"https://simple.wikipedia.org/wiki/Equalization_(audio)\">Wikipedia</a>, license: <a href=\"https://creativecommons.org/licenses/by/2.0/\">CC BY 2.0</a>\n",
    "</span></center>\n",
    "\n",
    "\n",
    "The following function realises one of the sliders in software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peaking_filter(gain,center_freq,q,fs):\n",
    "    \"\"\"\n",
    "    Derive coefficients for a peaking filter with a given amplitude and\n",
    "     bandwidth.  All coefficients are calculated as described in Zölzer's\n",
    "     DAFX book (ISBN: 0-471-49078-4, p. 50 - 55).  This algorithm assumes \n",
    "     a constant q-term is used through the equation.\n",
    "    \n",
    "    Usage:     `b,a` = peaking_filter(gain,center_freq, q,fs)\n",
    "                `gain` is the logarithmic gain (in dB)\n",
    "                `center_freq` is the center frequency\n",
    "                `q` is q-term equating to (Fb / Fc)\n",
    "                `fs` is the sampling rate\n",
    "    \n",
    "    Author:    Jeff Tackett 08/22/05\n",
    "    Port to Python by George Close 10/07/21\n",
    "    \"\"\"\n",
    "    \n",
    "    gain = np.float32(gain)\n",
    "    k = np.tan((np.pi*center_freq)/fs)\n",
    "    V0 = 10**((gain)/20)\n",
    "    # invert gain if a cut\n",
    "    if V0 < 1:\n",
    "        V0 = 1/V0\n",
    "\n",
    "    # Boost\n",
    "    if gain > 0:\n",
    "        b0 = (1 + ((V0/q)*k)+ k**2) / (1+((1/q)*k)+k**2)\n",
    "        b1 = (2 * (k**2 - 1)) / (1 + ((1/q)*k) + k**2);\n",
    "        b2 = (1 - ((V0/q)*k) + k**2) / (1 + ((1/q)*k) + k**2);\n",
    "        a1 = b1;\n",
    "        a2 =  (1 - ((1/q)*k) + k**2) / (1 + ((1/q)*k) + k**2);\n",
    "    # Cut\n",
    "    elif gain <0:\n",
    "        b0 = (1 + ((1/q)*k) + k**2) / (1 + ((V0/q)*k) + k**2);\n",
    "        b1 =       (2 * (k**2 - 1)) / (1 + ((V0/q)*k) + k**2);\n",
    "        b2 = (1 - ((1/q)*k) + k**2) / (1 + ((V0/q)*k) + k**2);\n",
    "        a1 = b1;\n",
    "        a2 = (1 - ((V0/q)*k) + k**2) / (1 + ((V0/q)*k) + k**2);\n",
    "    #gain is 0\n",
    "    else:\n",
    "        b0 = V0;\n",
    "        b1 = 0;\n",
    "        b2 = 0;\n",
    "        a1 = 0;\n",
    "        a2 = 0;\n",
    "    a = [  1, a1, a2];\n",
    "    b = [ b0, b1, b2];\n",
    "    return b,a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #abe;\">\n",
    "    \n",
    "**Task T17: (for COM4502-6502 only)**\n",
    "    \n",
    "* Visualise the frequency response of one filter.\n",
    "* Implement a cascade of filters to realise an equaliser.\n",
    "* Visualise the frequency response of your equaliser filter and the input and (filtered) output signal.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here \n",
    "#\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for submission\n",
    "\n",
    "<br>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #abe;\">\n",
    "    \n",
    "**Task T18:**\n",
    " \n",
    "* Clear all cell outputs to reduce the file size (in Jupyter Notebooks click on \"Cell->All Output->Clear\")\n",
    "* Create a `.zip` file named `YourName.zip` containing this Jupyter Notebook files as well as all other files necessary to run this notebook (**if such exist**, e.g. if you created (additional) WAVE files). \n",
    "* Hand in your `.zip` file via Blackboard.\n",
    "    \n",
    "\n",
    "<span style=\"font-weight:bold;color:red;text-align:center;\">**Important: For marking, we expect your code to work ‘out of the box’.**</span> This means that no additional software should need to be installed to make the Notebook run. If you only used libraries known from the Speech Processing Lab classes, you should be safe here.\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SP_final_assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
